{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KQpI7h0VNFxj",
    "outputId": "1996f9cb-be6f-484d-abd3-b6abd3f60ec9"
   },
   "outputs": [],
   "source": [
    "# %tensorflow_version 2.x\n",
    "# !pip uninstall -y tensorflow\n",
    "# !pip install tensorflow-gpu==1.15.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Os1YQiUmNBUH"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import timeit\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import griddata\n",
    "import random\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from glob import glob \n",
    "import xlwt\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2qclbFWvNBUJ"
   },
   "outputs": [],
   "source": [
    "class neural_net(object):\n",
    "    def __init__(self, *inputs, layers, monotonic = False, normalize = False, LAA = False):\n",
    "\n",
    "        self.layers = layers\n",
    "        self.num_layers = len(self.layers)\n",
    "        self.monotonic = monotonic\n",
    "        self.normalize = normalize\n",
    "        self.LAA = LAA\n",
    "        \n",
    "        if self.normalize:\n",
    "            if len(inputs) == 0:\n",
    "                in_dim = self.layers[0]\n",
    "                self.X_mean = np.zeros([1, in_dim])\n",
    "                self.X_std = np.ones([1, in_dim])\n",
    "            else:\n",
    "                X = np.concatenate(inputs, 1)\n",
    "                self.X_mean = X.mean(0, keepdims=True)\n",
    "                self.X_std = X.std(0, keepdims=True)\n",
    "\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        self.A = []\n",
    "\n",
    "        for l in range(0,self.num_layers-1):\n",
    "            in_dim = self.layers[l]\n",
    "            out_dim = self.layers[l+1]\n",
    "            xavier_stddev = np.sqrt(2/(in_dim + out_dim))\n",
    "            W = tf.Variable(tf.random.truncated_normal([in_dim, out_dim], stddev = xavier_stddev),dtype=tf.float32, trainable=True) \n",
    "            # monotonically increasing neural network\n",
    "            if self.monotonic:\n",
    "                W = W**2\n",
    "            b = tf.Variable(np.zeros([1, out_dim]), dtype=tf.float32, trainable=True)\n",
    "            \n",
    "            # tensorflow variables\n",
    "            self.weights.append(W)\n",
    "            self.biases.append(b)\n",
    "            \n",
    "            # locally adaptive activation\n",
    "            if self.LAA:\n",
    "                a = tf.Variable(0.05, dtype=tf.float32)\n",
    "                self.A.append(a)\n",
    "            \n",
    "    def __call__(self, *inputs):\n",
    "        if self.normalize:\n",
    "            H = (tf.concat(inputs, 1) - self.X_mean)/self.X_std\n",
    "        else:\n",
    "            H = tf.concat(inputs, 1)\n",
    "\n",
    "        for l in range(0, self.num_layers-1):\n",
    "            W = self.weights[l]\n",
    "            b = self.biases[l]\n",
    "            \n",
    "            # matrix multiplication\n",
    "            H = tf.matmul(H, W)\n",
    "            # add bias\n",
    "            H = H + b\n",
    "            # activation\n",
    "            if l < self.num_layers-2:\n",
    "                if self.LAA:\n",
    "                    H = tf.tanh(20 * self.A[l]*H)\n",
    "                else:\n",
    "                    H = tf.tanh(H)\n",
    "\n",
    "        return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wRRvIgeuNBUK"
   },
   "outputs": [],
   "source": [
    "def tf_session():\n",
    "    # tf session\n",
    "    config = tf.ConfigProto(allow_soft_placement=True,  # an operation might be placed on CPU for example if GPU is not available\n",
    "                            log_device_placement=True)  # print out device information\n",
    "    config.gpu_options.allow_growth = True\n",
    "    sess = tf.Session(config=config)\n",
    "\n",
    "    # init\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    return sess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kEle8eORNBUK"
   },
   "outputs": [],
   "source": [
    "def Gardner(psi, theta_r, theta_s, alpha, K_s):\n",
    "    K = K_s * tf.exp(alpha * psi)\n",
    "    theta = theta_r + (theta_s - theta_r) * tf.exp(alpha * psi)\n",
    "    return theta, K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_MF78l5mNBUL"
   },
   "outputs": [],
   "source": [
    "# theta_r, theta_s, alpha [/cm], K_s [cm/hour]\n",
    "# layer 1 is the lower layer while layer 2 is the upper layer\n",
    "Srivastava_parameters = {\n",
    "            \"layer1\": [0.06, 0.40, 1.0, 1.0],\n",
    "            \"layer2\": [0.06, 0.40, 1.0, 10.0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5jmWLJ_sNBUL",
    "outputId": "c12de995-5724-42ff-aef7-c5a339c83390"
   },
   "outputs": [],
   "source": [
    "sess = tf_session()\n",
    "\n",
    "log10_h = np.arange(-2, 6, 0.1)  # logarithm of suction (negative matric potential) in cm\n",
    "psi = -10**log10_h\n",
    "\n",
    "tf_psi = tf.constant(psi, dtype = tf.float32)\n",
    "\n",
    "tf_theta_1, tf_K_1 = Gardner(tf_psi, *Srivastava_parameters[\"layer1\"])\n",
    "tf_theta_2, tf_K_2 = Gardner(tf_psi, *Srivastava_parameters[\"layer2\"])\n",
    "psi = sess.run(tf_psi)\n",
    "theta_1 = sess.run(tf_theta_1)\n",
    "theta_2 = sess.run(tf_theta_2)\n",
    "K_1 = sess.run(tf_K_1)\n",
    "K_2 = sess.run(tf_K_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 316
    },
    "id": "HFuMhyt8NBUM",
    "outputId": "01e20f12-2385-45a6-ac4a-21b7c42472b6"
   },
   "outputs": [],
   "source": [
    "plt.plot(np.log10(-psi), theta_1, label = \"layer1\")\n",
    "plt.plot(np.log10(-psi), theta_2, label =  \"layer2\")\n",
    "plt.title(\"Water Retention Curve\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"log$_{10}(-\\\\psi)$\")\n",
    "plt.ylabel(\"$\\\\theta$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 316
    },
    "id": "05jWKQoYNBUM",
    "outputId": "9377db7d-996a-4f73-d2f9-e1c6249ac749"
   },
   "outputs": [],
   "source": [
    "plt.plot(np.log10(-psi), K_1, label = \"layer 1\")\n",
    "plt.plot(np.log10(-psi), K_2, label = \"layer 2\")\n",
    "plt.title(\"Hydraulic Conductivity Function\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"log$_{10}(-\\\\psi)$\")\n",
    "plt.ylabel(\"$K [cm / hour]$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wCf-WqiCNBUN"
   },
   "outputs": [],
   "source": [
    "def log10(x):\n",
    "    numerator = tf.log(x)\n",
    "    denominator = tf.log(tf.constant(10, dtype=numerator.dtype))\n",
    "    return numerator / denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TNbblRDhNBUN"
   },
   "outputs": [],
   "source": [
    "def RRE_mixed_1D(psi, theta, K, t, z):\n",
    "    \n",
    "    theta_t = tf.gradients(theta, t)[0]\n",
    "    theta_z = tf.gradients(theta, z)[0]\n",
    "    \n",
    "    psi_z = tf.gradients(psi, z)[0]\n",
    "    psi_zz = tf.gradients(psi_z, z)[0]\n",
    "    \n",
    "    K_z = tf.gradients(K, z)[0]\n",
    "\n",
    "    qz = -K*(psi_z + 1.0)\n",
    "    residual = theta_t - K_z*psi_z - K*psi_zz - K_z\n",
    "    return residual, qz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zt2cCuq2NBUO"
   },
   "outputs": [],
   "source": [
    "class Sampler:\n",
    "    # Initialize the class\n",
    "    def __init__(self, dim, coords, func, name = None):\n",
    "        self.dim = dim\n",
    "        self.coords = coords\n",
    "        self.func = func\n",
    "        self.name = name\n",
    "    def sample(self, N):\n",
    "        x = self.coords[0:1,:] + (self.coords[1:2,:]-self.coords[0:1,:])*np.random.rand(N, self.dim)\n",
    "        y = self.func(x)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "781Me2ywNBUO"
   },
   "outputs": [],
   "source": [
    "class PINNs_RRE1D(object):\n",
    "    \"\"\"\n",
    "    PINNs_RRE1D class implements physics-informed neural network solver for \n",
    "    Ricahrdson-Ricahrds equation using TensorFlow 1.15.\n",
    "    # notational conventions\n",
    "    _tf: placeholders\n",
    "    _pred: output of neural networks or other parametric models\n",
    "    _res: values related to residual points\n",
    "    _data: training data \n",
    "    _ic: initial condition\n",
    "    _bc: boundary condition (ub: upper; lb: lower for 1D)\n",
    "    _star: prediction (used after training)\n",
    "    _1: lower layer\n",
    "    _2: upper layer\n",
    "    _int: interface of the layers\n",
    "    t: time\n",
    "    z: vertical coordinate\n",
    "    theta: volumetric water content\n",
    "    psi: matric potential\n",
    "    K: hydraulic conductivity\n",
    "    ALR: adaptive learning rate\n",
    "    LAA: layer-wise locally adaptive activation functions\n",
    "    weight_values: [ic_1, lb, res_2, ic_2, ub, res_int, psi_int, qz_int]\n",
    "    \"\"\"\n",
    "    def __init__(self, t_res_1, z_res_1,\n",
    "                       t_res_2, z_res_2,\n",
    "                       t_int, z_int,\n",
    "                       t_ic_1, z_ic_1, theta_ic_1,\n",
    "                       t_ic_2, z_ic_2, theta_ic_2,\n",
    "                       t_ub, z_ub, qz_ub,\n",
    "                       t_lb, z_lb, theta_lb,\n",
    "                       layers_psi_1, layers_psi_2, hydraulic_model, \n",
    "                       hydraulic_parameters_1, hydraulic_parameters_2, weight_values,\n",
    "                       normalize, ALR, LAA):\n",
    "        \n",
    "        # NN architecture\n",
    "        self.layers_psi_1 = layers_psi_1\n",
    "        self.layers_psi_2 = layers_psi_2\n",
    "        \n",
    "        # Mode \n",
    "        self.hydraulic_model = hydraulic_model  # \"monotonic\", \"VGM\", \"Brooks\",\"PDI\", \"Gardner\"\n",
    "        self.normalize = normalize # normalize input values to neural networks\n",
    "        self.ALR = ALR  # adaptive learning rate\n",
    "        self.LAA = LAA  # locally adaptive activation functions\n",
    "        \n",
    "        # Adaptive re-weighting constant\n",
    "        self.rate = 0.9\n",
    "        \n",
    "        # initial values for weights in the loss function\n",
    "        \n",
    "        self.adaptive_constant_ic_1_val = np.array(weight_values[0])\n",
    "        self.adaptive_constant_lb_val = np.array(weight_values[1])\n",
    "        \n",
    "        self.adaptive_constant_res_2_val = np.array(weight_values[2])\n",
    "        self.adaptive_constant_ic_2_val = np.array(weight_values[3])\n",
    "        self.adaptive_constant_ub_val = np.array(weight_values[4])\n",
    "        \n",
    "        self.adaptive_constant_res_int_val = np.array(weight_values[5])\n",
    "        self.adaptive_constant_psi_int_val = np.array(weight_values[6])\n",
    "        self.adaptive_constant_qz_int_val = np.array(weight_values[7])\n",
    "        \n",
    "        # hydraulic parameters  \n",
    "                    \n",
    "        if self.hydraulic_model == \"Gardner\":  # theta_r, theta_s, alpha, K_s\n",
    "            self.theta_r_1 = tf.Variable(hydraulic_parameters_1[0], dtype=tf.float32, trainable=False)\n",
    "            self.theta_s_1 = tf.Variable(hydraulic_parameters_1[1], dtype=tf.float32, trainable=False)\n",
    "            self.alpha_1 = tf.Variable(hydraulic_parameters_1[2], dtype=tf.float32, trainable=False)\n",
    "            self.K_s_1 = tf.Variable(hydraulic_parameters_1[3], dtype=tf.float32, trainable=False)\n",
    "            \n",
    "            self.theta_r_2 = tf.Variable(hydraulic_parameters_2[0], dtype=tf.float32, trainable=False)\n",
    "            self.theta_s_2 = tf.Variable(hydraulic_parameters_2[1], dtype=tf.float32, trainable=False)\n",
    "            self.alpha_2 = tf.Variable(hydraulic_parameters_2[2], dtype=tf.float32, trainable=False)\n",
    "            self.K_s_2 = tf.Variable(hydraulic_parameters_2[3], dtype=tf.float32, trainable=False)\n",
    "\n",
    "        # data\n",
    "        \n",
    "        [self.t_res_1, self.z_res_1] = [t_res_1, z_res_1]\n",
    "        [self.t_res_2, self.z_res_2] = [t_res_2, z_res_2]\n",
    "        [self.t_int, self.z_int] = [t_int, z_int]\n",
    "        [self.t_ic_1, self.z_ic_1, self.theta_ic_1] = [t_ic_1, z_ic_1, theta_ic_1]\n",
    "        [self.t_ic_2, self.z_ic_2, self.theta_ic_2] = [t_ic_2, z_ic_2, theta_ic_2]\n",
    "        [self.t_ub, self.z_ub, self.qz_ub] = [t_ub, z_ub, qz_ub]\n",
    "        [self.t_lb, self.z_lb, self.theta_lb] = [t_lb, z_lb, theta_lb]\n",
    "        \n",
    "        # placeholders\n",
    "        \n",
    "        [self.t_res_1_tf, self.z_res_1_tf] = [tf.placeholder(tf.float32, shape=[None, 1]) for _ in range(2)]\n",
    "        [self.t_res_2_tf, self.z_res_2_tf] = [tf.placeholder(tf.float32, shape=[None, 1]) for _ in range(2)]\n",
    "        [self.t_int_tf, self.z_int_tf] = [tf.placeholder(tf.float32, shape=[None, 1]) for _ in range(2)]\n",
    "\n",
    "        [self.t_ic_1_tf, self.z_ic_1_tf, self.theta_ic_1_tf] = [tf.placeholder(tf.float32, shape=[None, 1]) for _ in range(3)]\n",
    "        [self.t_ic_2_tf, self.z_ic_2_tf, self.theta_ic_2_tf] = [tf.placeholder(tf.float32, shape=[None, 1]) for _ in range(3)]\n",
    "\n",
    "        [self.t_ub_tf, self.z_ub_tf, self.qz_ub_tf] = [tf.placeholder(tf.float32, shape=[None, 1]) for _ in range(3)]\n",
    "        [self.t_lb_tf, self.z_lb_tf, self.theta_lb_tf] = [tf.placeholder(tf.float32, shape=[None, 1]) for _ in range(3)]\n",
    "        \n",
    "        \n",
    "        # weight parameters in the loss function\n",
    "        self.adaptive_constant_ic_1_tf = tf.placeholder(tf.float32, shape=self.adaptive_constant_ic_1_val.shape)\n",
    "        self.adaptive_constant_lb_tf = tf.placeholder(tf.float32, shape=self.adaptive_constant_lb_val.shape)\n",
    "        \n",
    "        self.adaptive_constant_res_2_tf = tf.placeholder(tf.float32, shape=self.adaptive_constant_res_2_val.shape)\n",
    "        self.adaptive_constant_ic_2_tf = tf.placeholder(tf.float32, shape=self.adaptive_constant_ic_2_val.shape)\n",
    "        self.adaptive_constant_ub_tf = tf.placeholder(tf.float32, shape=self.adaptive_constant_ub_val.shape)\n",
    "        self.adaptive_constant_res_int_tf = tf.placeholder(tf.float32, shape=self.adaptive_constant_res_int_val.shape)\n",
    "        self.adaptive_constant_psi_int_tf = tf.placeholder(tf.float32, shape=self.adaptive_constant_psi_int_val.shape)\n",
    "        self.adaptive_constant_qz_int_tf = tf.placeholder(tf.float32, shape=self.adaptive_constant_qz_int_val.shape)\n",
    "        \n",
    "        # neural network definition\n",
    "        self.net_psi_1 = neural_net(self.t_res_1, self.z_res_1, layers = self.layers_psi_1, monotonic = False, normalize = self.normalize, LAA = self.LAA)\n",
    "        self.net_psi_2 = neural_net(self.t_res_2, self.z_res_2, layers = self.layers_psi_2, monotonic = False, normalize = self.normalize, LAA = self.LAA)\n",
    "     \n",
    "        # prediction at initial condition\n",
    "        \n",
    "        self.log_psi_ic_1_pred = self.net_psi_1(self.t_ic_1_tf, self.z_ic_1_tf)\n",
    "        self.psi_ic_1_pred = -tf.exp(self.log_psi_ic_1_pred) + 1.0\n",
    "        self.theta_ic_1_pred, self.K_ic_1_pred = Gardner(self.psi_ic_1_pred, self.theta_r_1, self.theta_s_1, self.alpha_1, self.K_s_1)\n",
    "\n",
    "        self.log_psi_ic_2_pred = self.net_psi_2(self.t_ic_2_tf, self.z_ic_2_tf)\n",
    "        self.psi_ic_2_pred = -tf.exp(self.log_psi_ic_2_pred) + 1.0\n",
    "        self.theta_ic_2_pred, self.K_ic_2_pred = Gardner(self.psi_ic_2_pred, self.theta_r_2, self.theta_s_2, self.alpha_2, self.K_s_2)\n",
    "        \n",
    "        # prediction at upper boundary\n",
    "        self.log_psi_ub_pred = self.net_psi_2(self.t_ub_tf, self.z_ub_tf)\n",
    "        self.psi_ub_pred = -tf.exp(self.log_psi_ub_pred) + 1.0\n",
    "        self.theta_ub_pred, self.K_ub_pred = Gardner(self.psi_ub_pred, self.theta_r_2, self.theta_s_2, self.alpha_2, self.K_s_2)\n",
    "        \n",
    "        self.qz_ub_pred = RRE_mixed_1D(self.psi_ub_pred, self.theta_ub_pred, self.K_ub_pred,\n",
    "                                                                self.t_ub_tf, self.z_ub_tf)[1]\n",
    "            \n",
    "           \n",
    "        # prediction at lower boundary\n",
    "        self.log_psi_lb_pred = self.net_psi_1(self.t_lb_tf, self.z_lb_tf)\n",
    "        self.psi_lb_pred = -tf.exp(self.log_psi_lb_pred) + 1.0\n",
    "        self.theta_lb_pred, self.K_lb_pred = Gardner(self.psi_lb_pred, self.theta_r_1, self.theta_s_1, self.alpha_1, self.K_s_1)\n",
    "        \n",
    "        # prediction at residual points\n",
    "        self.log_psi_res_1_pred = self.net_psi_1(self.t_res_1_tf, self.z_res_1_tf)\n",
    "        self.psi_res_1_pred = -tf.exp(self.log_psi_res_1_pred) + 1.0\n",
    "        self.theta_res_1_pred, self.K_res_1_pred = Gardner(self.psi_res_1_pred, self.theta_r_1, self.theta_s_1, self.alpha_1, self.K_s_1)\n",
    "              \n",
    "        self.residual_res_1_pred, self.qz_res_1_pred = RRE_mixed_1D(self.psi_res_1_pred, self.theta_res_1_pred, self.K_res_1_pred,\n",
    "                                                                self.t_res_1_tf, self.z_res_1_tf)\n",
    "        \n",
    "        self.log_psi_res_2_pred = self.net_psi_2(self.t_res_2_tf, self.z_res_2_tf)\n",
    "        self.psi_res_2_pred = -tf.exp(self.log_psi_res_2_pred) + 1.0\n",
    "        self.theta_res_2_pred, self.K_res_2_pred = Gardner(self.psi_res_2_pred, self.theta_r_2, self.theta_s_2, self.alpha_2, self.K_s_2)\n",
    "              \n",
    "        self.residual_res_2_pred, self.qz_res_2_pred = RRE_mixed_1D(self.psi_res_2_pred, self.theta_res_2_pred, self.K_res_2_pred,\n",
    "                                                                self.t_res_2_tf, self.z_res_2_tf)\n",
    "\n",
    "        # psi, flux, and residual at the layer interface\n",
    "        # layer 1\n",
    "        self.log_psi_int_1_pred = self.net_psi_1(self.t_int_tf, self.z_int_tf)\n",
    "        self.psi_int_1_pred = -tf.exp(self.log_psi_int_1_pred) + 1.0\n",
    "        self.theta_int_1_pred, self.K_int_1_pred = Gardner(self.psi_int_1_pred, self.theta_r_1, self.theta_s_1, self.alpha_1, self.K_s_1)\n",
    "              \n",
    "        self.residual_int_1_pred, self.qz_int_1_pred = RRE_mixed_1D(self.psi_int_1_pred, self.theta_int_1_pred, self.K_int_1_pred,\n",
    "                                                                self.t_int_tf, self.z_int_tf)\n",
    "        \n",
    "        # layer 2\n",
    "        self.log_psi_int_2_pred = self.net_psi_2(self.t_int_tf, self.z_int_tf)\n",
    "        self.psi_int_2_pred = -tf.exp(self.log_psi_int_2_pred) + 1.0\n",
    "        self.theta_int_2_pred, self.K_int_2_pred = Gardner(self.psi_int_2_pred, self.theta_r_2, self.theta_s_2, self.alpha_2, self.K_s_2)\n",
    "              \n",
    "        self.residual_int_2_pred, self.qz_int_2_pred = RRE_mixed_1D(self.psi_int_2_pred, self.theta_int_2_pred, self.K_int_2_pred,\n",
    "                                                                self.t_int_tf, self.z_int_tf)\n",
    "                \n",
    "        # loss\n",
    "        # layer 1\n",
    "        self.loss_res_1 =  tf.reduce_mean(tf.square(self.residual_res_1_pred))\n",
    "        self.loss_ic_1 = tf.reduce_mean(tf.square(self.theta_ic_1_pred - self.theta_ic_1_tf))\n",
    "        self.loss_lb = tf.reduce_mean(tf.square(self.theta_lb_pred - self.theta_lb_tf))\n",
    "        \n",
    "        self.loss_1 = self.loss_res_1 +  self.adaptive_constant_ic_1_tf * self.loss_ic_1  \\\n",
    "                                      +  self.adaptive_constant_lb_tf * self.loss_lb\n",
    "        \n",
    "        # layer 2\n",
    "        self.loss_res_2 =  tf.reduce_mean(tf.square(self.residual_res_2_pred))\n",
    "        self.loss_ic_2 = tf.reduce_mean(tf.square(self.theta_ic_2_pred - self.theta_ic_2_tf))\n",
    "        self.loss_ub = tf.reduce_mean(tf.square(self.qz_ub_pred - self.qz_ub_tf))\n",
    "        \n",
    "        self.loss_2 = self.adaptive_constant_res_2_tf * self.loss_res_2 +  self.adaptive_constant_ic_2_tf * self.loss_ic_2  \\\n",
    "                                      +  self.adaptive_constant_ub_tf * self.loss_ub\n",
    "        \n",
    "        self.loss_res_int = tf.reduce_mean(tf.square(self.residual_int_1_pred - self.residual_int_2_pred))\n",
    "        self.loss_psi_int = tf.reduce_mean(tf.square(self.log_psi_int_1_pred - self.log_psi_int_2_pred))\n",
    "        self.loss_qz_int = tf.reduce_mean(tf.square(self.qz_int_1_pred - self.qz_int_2_pred))\n",
    "        \n",
    "        self.loss_int = self.adaptive_constant_res_int_tf * self.loss_res_int  \\\n",
    "                         +  self.adaptive_constant_psi_int_tf * self.loss_psi_int  \\\n",
    "                         +  self.adaptive_constant_qz_int_tf * self.loss_qz_int\n",
    "        \n",
    "        \n",
    "        self.loss = self.loss_1 + self.loss_2 + self.loss_int\n",
    "        \n",
    "                    \n",
    "        # Define optimizer with learning rate schedule\n",
    "        self.global_step = tf.Variable(0, trainable = False)\n",
    "        starter_learning_rate = 1e-3\n",
    "        self.learning_rate = tf.train.exponential_decay(starter_learning_rate, self.global_step,\n",
    "                                                        1000, 0.90, staircase=False)\n",
    "        \n",
    "        # Passing global_step to minimize() will increment it at each step.\n",
    "        # self.train_op_1 = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss_1, global_step=self.global_step)\n",
    "        # self.train_op_2 = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss_2, global_step=self.global_step)\n",
    "        self.train_op = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss, global_step=self.global_step)\n",
    "        \n",
    "\n",
    "        # L-BFGS-B method\n",
    "        self.optimizer = tf.contrib.opt.ScipyOptimizerInterface(self.loss,\n",
    "                                                        method = 'L-BFGS-B',\n",
    "                                                        options = {'maxiter': 50000,\n",
    "                                                                   'maxfun': 50000,\n",
    "                                                                   'maxcor': 50,\n",
    "                                                                   'maxls': 50,\n",
    "                                                                   'ftol' : 1e-10,\n",
    "                                                                   'gtol' : 1e-8})\n",
    "        \n",
    "        \n",
    "        # Logger\n",
    "        \n",
    "        self.loss_res_1_log = []\n",
    "        self.loss_ic_1_log = []\n",
    "        self.loss_lb_log = []\n",
    "        self.loss_res_2_log = []\n",
    "        self.loss_ic_2_log = []\n",
    "        self.loss_ub_log = []\n",
    "        self.loss_psi_int_log = []\n",
    "        self.loss_qz_int_log = []\n",
    "        self.loss_res_int_log = []\n",
    "        \n",
    "        self.loss_res_1_log_BFGS = []\n",
    "        self.loss_ic_1_log_BFGS = []\n",
    "        self.loss_lb_log_BFGS = []\n",
    "        self.loss_res_2_log_BFGS = []\n",
    "        self.loss_ic_2_log_BFGS = []\n",
    "        self.loss_ub_log_BFGS = []\n",
    "        self.loss_psi_int_log_BFGS = []\n",
    "        self.loss_qz_int_log_BFGS = []\n",
    "        self.loss_res_int_log_BFGS = []\n",
    "        \n",
    "        self.saver = tf.train.Saver()\n",
    "        \n",
    "        # new residual points\n",
    "        self.new_residual_1 = []\n",
    "        self.new_residual_2 = []\n",
    "        \n",
    "        # Store the adaptive constant\n",
    "        \n",
    "        self.adaptive_constant_ic_1_log = []\n",
    "        self.adaptive_constant_lb_log = []\n",
    "        \n",
    "        self.adaptive_constant_res_2_log = []\n",
    "        self.adaptive_constant_ic_2_log = []\n",
    "        self.adaptive_constant_ub_log = []\n",
    "        \n",
    "        self.adaptive_constant_res_int_log = []\n",
    "        self.adaptive_constant_psi_int_log = []\n",
    "        self.adaptive_constant_qz_int_log = []\n",
    "        \n",
    "                \n",
    "        # Compute the adaptive constant\n",
    "        \n",
    "        self.adaptive_constant_ic_1_list = []\n",
    "        self.adaptive_constant_lb_list = []\n",
    "        \n",
    "        self.adaptive_constant_res_2_list = []\n",
    "        self.adaptive_constant_ic_2_list = []\n",
    "        self.adaptive_constant_ub_list = []\n",
    "        \n",
    "        self.adaptive_constant_res_int_list = []\n",
    "        self.adaptive_constant_psi_int_list = []\n",
    "        self.adaptive_constant_qz_int_list = []\n",
    "        \n",
    "        \n",
    "        self.sess = tf_session()\n",
    "\n",
    "    def train_Adam(self, iterations, batch = True, batch_size = 128):\n",
    "\n",
    "        N_res_1 = self.t_res_1.shape[0]\n",
    "        N_res_2 = self.t_res_2.shape[0]\n",
    "\n",
    "        start_time = timeit.default_timer()\n",
    "        running_time = 0\n",
    "        it = 0\n",
    "        for it in range(iterations):\n",
    "           \n",
    "            if batch:\n",
    "\n",
    "                idx_res_1 = np.random.choice(N_res_1, batch_size, replace = False)\n",
    "                idx_res_2 = np.random.choice(N_res_2, batch_size, replace = False)\n",
    "\n",
    "                (t_res_1, z_res_1) = (self.t_res_1[idx_res_1,:],\n",
    "                                  self.z_res_1[idx_res_1,:])\n",
    "                (t_res_2, z_res_2) = (self.t_res_2[idx_res_2,:],\n",
    "                                  self.z_res_2[idx_res_2,:])\n",
    "                \n",
    "            else:\n",
    "\n",
    "                (t_res_1, z_res_1) = (self.t_res_1,\n",
    "                                  self.z_res_1)\n",
    "                (t_res_2, z_res_2) = (self.t_res_2,\n",
    "                                  self.z_res_2)\n",
    "\n",
    "            tf_dict = {self.t_res_1_tf: t_res_1,\n",
    "                       self.z_res_1_tf: z_res_1,\n",
    "                       self.t_res_2_tf: t_res_2,\n",
    "                       self.z_res_2_tf: z_res_2,\n",
    "                       self.t_int_tf: self.t_int,\n",
    "                       self.z_int_tf: self.z_int,\n",
    "                       self.t_ic_1_tf: self.t_ic_1,\n",
    "                       self.z_ic_1_tf: self.z_ic_1,\n",
    "                       self.theta_ic_1_tf: self.theta_ic_1,\n",
    "                       self.t_ic_2_tf: self.t_ic_2,\n",
    "                       self.z_ic_2_tf: self.z_ic_2,\n",
    "                       self.theta_ic_2_tf: self.theta_ic_2,\n",
    "                       self.t_ub_tf: self.t_ub,\n",
    "                       self.z_ub_tf: self.z_ub,\n",
    "                       self.qz_ub_tf: self.qz_ub,\n",
    "                       self.t_lb_tf: self.t_lb,\n",
    "                       self.z_lb_tf: self.z_lb,\n",
    "                       self.theta_lb_tf: self.theta_lb,\n",
    "                       self.adaptive_constant_ic_1_tf: self.adaptive_constant_ic_1_val,\n",
    "                       self.adaptive_constant_lb_tf: self.adaptive_constant_lb_val,\n",
    "                       self.adaptive_constant_res_2_tf: self.adaptive_constant_res_2_val,\n",
    "                       self.adaptive_constant_ic_2_tf: self.adaptive_constant_ic_2_val,\n",
    "                       self.adaptive_constant_ub_tf: self.adaptive_constant_ub_val, \n",
    "                       self.adaptive_constant_res_int_tf: self.adaptive_constant_res_int_val,\n",
    "                       self.adaptive_constant_psi_int_tf: self.adaptive_constant_psi_int_val,\n",
    "                       self.adaptive_constant_qz_int_tf: self.adaptive_constant_qz_int_val}\n",
    "        \n",
    "        \n",
    "#             self.sess.run(self.train_op_1, tf_dict)\n",
    "#             self.sess.run(self.train_op_2, tf_dict)\n",
    "            self.sess.run(self.train_op, tf_dict)\n",
    "    \n",
    "            # Print\n",
    "            if it % 10 == 0:\n",
    "                elapsed = timeit.default_timer() - start_time\n",
    "                running_time += elapsed/3600.0\n",
    "                \n",
    "                \n",
    "        \n",
    "        \n",
    "                loss_res_1_value, loss_ic_1_value, loss_lb_value = self.sess.run([self.loss_res_1, self.loss_ic_1, self.loss_lb], tf_dict)\n",
    "                loss_res_2_value, loss_ic_2_value, loss_ub_value = self.sess.run([self.loss_res_2, self.loss_ic_2, self.loss_ub], tf_dict)\n",
    "\n",
    "                loss_psi_int_value, loss_qz_int_value, loss_res_int_value = self.sess.run([self.loss_psi_int, self.loss_qz_int, self.loss_res_int], tf_dict)\n",
    "                                                                                               \n",
    "                self.loss_res_1_log.append(loss_res_1_value)\n",
    "                self.loss_ic_1_log.append(loss_ic_1_value)\n",
    "                self.loss_lb_log.append(loss_lb_value)\n",
    "                self.loss_res_2_log.append(loss_res_2_value)\n",
    "                self.loss_ic_2_log.append(loss_ic_2_value)\n",
    "                self.loss_ub_log.append(loss_ub_value)\n",
    "                self.loss_psi_int_log.append(loss_psi_int_value)\n",
    "                self.loss_qz_int_log.append(loss_qz_int_value)\n",
    "                self.loss_res_int_log.append(loss_res_int_value)   \n",
    "                \n",
    "                print('It: %d, Loss_res_1: %.3e, Loss_ic_1: %.3e, Loss_lb: %.3e,  \\\n",
    "                               Loss_res_2: %.3e, Loss_ic_2: %.3e, Loss_ub: %.3e,  \\\n",
    "                               Loss_psi_int: %.3e, Loss_qz_int: %.3e, Loss_res_int: %.3e, Time: %.2f' %\n",
    "                      (it, loss_res_1_value, loss_ic_1_value, loss_lb_value,\n",
    "                           loss_res_2_value, loss_ic_2_value, loss_ub_value,\n",
    "                           loss_psi_int_value, loss_qz_int_value, loss_res_int_value, elapsed))\n",
    "                \n",
    "                start_time = timeit.default_timer()\n",
    "                                               \n",
    "    \n",
    "    def train_BFGS(self):\n",
    "        # full batch L-BFGS\n",
    "        tf_dict = {self.t_res_1_tf: self.t_res_1,\n",
    "                       self.z_res_1_tf: self.z_res_1,\n",
    "                       self.t_res_2_tf: self.t_res_2,\n",
    "                       self.z_res_2_tf: self.z_res_2,\n",
    "                       self.t_int_tf: self.t_int,\n",
    "                       self.z_int_tf: self.z_int,\n",
    "                       self.t_ic_1_tf: self.t_ic_1,\n",
    "                       self.z_ic_1_tf: self.z_ic_1,\n",
    "                       self.theta_ic_1_tf: self.theta_ic_1,\n",
    "                       self.t_ic_2_tf: self.t_ic_2,\n",
    "                       self.z_ic_2_tf: self.z_ic_2,\n",
    "                       self.theta_ic_2_tf: self.theta_ic_2,\n",
    "                       self.t_ub_tf: self.t_ub,\n",
    "                       self.z_ub_tf: self.z_ub,\n",
    "                       self.qz_ub_tf: self.qz_ub,\n",
    "                       self.t_lb_tf: self.t_lb,\n",
    "                       self.z_lb_tf: self.z_lb,\n",
    "                       self.theta_lb_tf: self.theta_lb,\n",
    "                       self.adaptive_constant_ic_1_tf: self.adaptive_constant_ic_1_val,\n",
    "                       self.adaptive_constant_lb_tf: self.adaptive_constant_lb_val,\n",
    "                       self.adaptive_constant_res_2_tf: self.adaptive_constant_res_2_val,\n",
    "                       self.adaptive_constant_ic_2_tf: self.adaptive_constant_ic_2_val,\n",
    "                       self.adaptive_constant_ub_tf: self.adaptive_constant_ub_val, \n",
    "                       self.adaptive_constant_res_int_tf: self.adaptive_constant_res_int_val,\n",
    "                       self.adaptive_constant_psi_int_tf: self.adaptive_constant_psi_int_val,\n",
    "                       self.adaptive_constant_qz_int_tf: self.adaptive_constant_qz_int_val}\n",
    "        \n",
    "#         L-BFGS-B\n",
    "        self.optimizer.minimize(self.sess,\n",
    "                                feed_dict = tf_dict,\n",
    "                                fetches = [self.loss_res_1, self.loss_ic_1, self.loss_lb,\n",
    "                                           self.loss_res_2, self.loss_ic_2, self.loss_ub,\n",
    "                                           self.loss_psi_int, self.loss_qz_int, self.loss_res_int],\n",
    "                                loss_callback = self.callback)\n",
    "        \n",
    "    def callback(self, loss_res_1_value, loss_ic_1_value, loss_lb_value,\n",
    "                           loss_res_2_value, loss_ic_2_value, loss_ub_value,\n",
    "                           loss_psi_int_value, loss_qz_int_value, loss_res_int_value):\n",
    "        \n",
    "        tf_dict = {self.t_res_1_tf: self.t_res_1,\n",
    "                       self.z_res_1_tf: self.z_res_1,\n",
    "                       self.t_res_2_tf: self.t_res_2,\n",
    "                       self.z_res_2_tf: self.z_res_2,\n",
    "                       self.t_int_tf: self.t_int,\n",
    "                       self.z_int_tf: self.z_int,\n",
    "                       self.t_ic_1_tf: self.t_ic_1,\n",
    "                       self.z_ic_1_tf: self.z_ic_1,\n",
    "                       self.theta_ic_1_tf: self.theta_ic_1,\n",
    "                       self.t_ic_2_tf: self.t_ic_2,\n",
    "                       self.z_ic_2_tf: self.z_ic_2,\n",
    "                       self.theta_ic_2_tf: self.theta_ic_2,\n",
    "                       self.t_ub_tf: self.t_ub,\n",
    "                       self.z_ub_tf: self.z_ub,\n",
    "                       self.qz_ub_tf: self.qz_ub,\n",
    "                       self.t_lb_tf: self.t_lb,\n",
    "                       self.z_lb_tf: self.z_lb,\n",
    "                       self.theta_lb_tf: self.theta_lb,\n",
    "                       self.adaptive_constant_ic_1_tf: self.adaptive_constant_ic_1_val,\n",
    "                       self.adaptive_constant_lb_tf: self.adaptive_constant_lb_val,\n",
    "                       self.adaptive_constant_res_2_tf: self.adaptive_constant_res_2_val,\n",
    "                       self.adaptive_constant_ic_2_tf: self.adaptive_constant_ic_2_val,\n",
    "                       self.adaptive_constant_ub_tf: self.adaptive_constant_ub_val, \n",
    "                       self.adaptive_constant_res_int_tf: self.adaptive_constant_res_int_val,\n",
    "                       self.adaptive_constant_psi_int_tf: self.adaptive_constant_psi_int_val,\n",
    "                       self.adaptive_constant_qz_int_tf: self.adaptive_constant_qz_int_val}\n",
    "        \n",
    "        self.loss_res_1_log_BFGS.append(loss_res_1_value)\n",
    "        self.loss_ic_1_log_BFGS.append(loss_ic_1_value)\n",
    "        self.loss_lb_log_BFGS.append(loss_lb_value)\n",
    "        self.loss_res_2_log_BFGS.append(loss_res_2_value)\n",
    "        self.loss_ic_2_log_BFGS.append(loss_ic_2_value)\n",
    "        self.loss_ub_log_BFGS.append(loss_ub_value)\n",
    "        self.loss_psi_int_log_BFGS.append(loss_psi_int_value)\n",
    "        self.loss_qz_int_log_BFGS.append(loss_qz_int_value)\n",
    "        self.loss_res_int_log_BFGS.append(loss_res_int_value)   \n",
    "                \n",
    "                \n",
    "        print('Loss_res_1: %.3e, Loss_ic_1: %.3e, Loss_lb: %.3e,  \\n\\\n",
    "                               Loss_res_2: %.3e, Loss_ic_2: %.3e, Loss_ub: %.3e,  \\n\\\n",
    "                               Loss_psi_int: %.3e, Loss_qz_int: %.3e, Loss_res_int: %.3e' %\n",
    "                      (loss_res_1_value, loss_ic_1_value, loss_lb_value,\n",
    "                           loss_res_2_value, loss_ic_2_value, loss_ub_value,\n",
    "                           loss_psi_int_value, loss_qz_int_value, loss_res_int_value))\n",
    "        \n",
    "        \n",
    "    def RAR(self, dom_coords_1, dom_coords_2, iterations, batchsize, m):\n",
    "        self.train_Adam(iterations, batchsize)\n",
    "        \n",
    "        i = 1\n",
    "        \n",
    "        while i < 10:\n",
    "            res_sampler_1 = Sampler(2, dom_coords_1, lambda x: 0)\n",
    "            res_sampler_2 = Sampler(2, dom_coords_2, lambda x: 0)\n",
    "            Z_res_1, _ = res_sampler_1.sample(np.int32(1e6))\n",
    "            Z_res_2, _ = res_sampler_2.sample(np.int32(1e6))\n",
    "            z_res_1, t_res_1 = Z_res_1[:, 0:1], Z_res_1[:, 1:2]\n",
    "            z_res_2, t_res_2 = Z_res_2[:, 0:1], Z_res_2[:, 1:2]\n",
    "        \n",
    "            tf_dict = {self.t_res_1_tf: t_res_1,\n",
    "                       self.z_res_1_tf: z_res_1,\n",
    "                       self.t_res_2_tf: t_res_2,\n",
    "                       self.z_res_2_tf: z_res_2}\n",
    "            residual_1 = self.sess.run(self.residual_res_1_pred, tf_dict)\n",
    "            residual_2 = self.sess.run(self.residual_res_2_pred, tf_dict)\n",
    "\n",
    "            err_abs_1 = np.abs(residual_1)\n",
    "            err_abs_2 = np.abs(residual_2)\n",
    "            err_1 = np.mean(err_abs_1)\n",
    "            err_2 = np.mean(err_abs_2)\n",
    "            print(\"Mean residual for layer 1: %.3e, Mean residual for layer 2: %.3e\" % (err_1, err_2))\n",
    "\n",
    "            # choose m largest residual\n",
    "            z_id_1 = err_abs_1.argsort(axis = 0)[-m * i:][::-1][:,0]\n",
    "            z_id_2 = err_abs_2.argsort(axis = 0)[-m * i:][::-1][:,0]\n",
    "\n",
    "            print(z_id_1, z_id_2)\n",
    "            print(\"Adding new point for layer 1:\", Z_res_1[z_id_1, :], \"\\n\")\n",
    "            print(\"High residuals for layer 1 are:\", err_abs_1[z_id_1], \"\\n\")\n",
    "            print(\"Adding new point for layer 2:\", Z_res_2[z_id_2, :], \"\\n\")\n",
    "            print(\"High residuals for layer 2 are:\", err_abs_2[z_id_2], \"\\n\")\n",
    "        \n",
    "            self.new_residual_1.append(Z_res_1[z_id_1, :])\n",
    "            self.new_residual_2.append(Z_res_2[z_id_2, :])\n",
    "            \n",
    "            self.z_res_1 = np.vstack((self.z_res_1, Z_res_1[z_id_1, 0:1]))\n",
    "            self.t_res_1 = np.vstack((self.t_res_1, Z_res_1[z_id_1, 1:2]))\n",
    "            \n",
    "            self.z_res_2 = np.vstack((self.z_res_2, Z_res_2[z_id_2, 0:1]))\n",
    "            self.t_res_2 = np.vstack((self.t_res_2, Z_res_2[z_id_2, 1:2]))\n",
    "            \n",
    "            self.train_Adam(iterations, batchsize)\n",
    "            \n",
    "            i += 1\n",
    "\n",
    "            print(i)\n",
    "            \n",
    "    def predict_1(self, t_star, z_star):\n",
    "\n",
    "        tf_dict = {self.t_res_1_tf: t_star, self.z_res_1_tf: z_star}\n",
    "\n",
    "        psi_star = self.sess.run(self.psi_res_1_pred, tf_dict)\n",
    "        theta_star = self.sess.run(self.theta_res_1_pred, tf_dict)\n",
    "        K_star = self.sess.run(self.K_res_1_pred, tf_dict)\n",
    "        qz_star = self.sess.run(self.qz_res_1_pred, tf_dict)\n",
    "        \n",
    "        residual = self.sess.run(self.residual_res_1_pred, tf_dict)\n",
    "\n",
    "        return psi_star, theta_star, K_star, qz_star, residual\n",
    "    \n",
    "    def predict_2(self, t_star, z_star):\n",
    "\n",
    "        tf_dict = {self.t_res_2_tf: t_star, self.z_res_2_tf: z_star}\n",
    "\n",
    "        psi_star = self.sess.run(self.psi_res_2_pred, tf_dict)\n",
    "        theta_star = self.sess.run(self.theta_res_2_pred, tf_dict)\n",
    "        K_star = self.sess.run(self.K_res_2_pred, tf_dict)\n",
    "        qz_star = self.sess.run(self.qz_res_2_pred, tf_dict)\n",
    "        \n",
    "        residual = self.sess.run(self.residual_res_2_pred, tf_dict)\n",
    "\n",
    "        return psi_star, theta_star, K_star, qz_star, residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cd drive/My\\ Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cd UFnets/forward_heterogeneous/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dom_coords_1 = np.array([[0.0, 0.0],   # t = 0 hour to t = 10 hours\n",
    "#                        [10.0, -10.0]])\n",
    "# dom_coords_2 = np.array([[0.0, 10.0],   # t = 0 hour to t = 10 hours\n",
    "#                        [10.0, 0.0]])\n",
    "# int_coords = np.array([[0.0, 0.0],\n",
    "#                        [10.0, 0.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# produce residual points \n",
    "# res_sampler_1 = Sampler(2, dom_coords_1, lambda x: 0)\n",
    "# res_sampler_2 = Sampler(2, dom_coords_2, lambda x: 0)\n",
    "# int_sampler = Sampler(2, int_coords, lambda x: 0)\n",
    "# np.random.seed(0)\n",
    "# Z_res_1, _ = res_sampler_1.sample(np.int32(100000))\n",
    "# Z_res_2, _ = res_sampler_2.sample(np.int32(100000))\n",
    "# Z_int, _ = int_sampler.sample(np.int32(10000))\n",
    "# np.save(\"data/residual_points_layer1\", Z_res_1)\n",
    "# np.save(\"data/residual_points_layer2\", Z_res_2)\n",
    "# np.save(\"data/residual_points_interface\", Z_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_loop(folder, ID, random_seed, number_int, LAA = True, ALR = False, weight_values = [10, 10, 10, 10, 10, 10, 10, 10],\n",
    "              number_layers_1 = 5, number_units_1 = 50, number_layers_2 = 5, number_units_2 = 50, \n",
    "              number_res_1 = 10000, number_res_2 = 10000, number_ub = 1000,\n",
    "              RAR = False, iterations = 100000, batch_size = 128, new_residual_points = 10):\n",
    "    \"\"\"\n",
    "    ALR: adaptive learning rate\n",
    "    LAA: layer-wise locally adaptive activation functions\n",
    "    RAR: residual adaptive refinement\n",
    "    weight_values: [ic_1, lb, res_2, ic_2, ub, res_int, psi_int, qz_int]\n",
    "    \"\"\"\n",
    "    \n",
    "    if not os.path.isdir(f'../results/{folder}'):\n",
    "        os.mkdir(f'../results/{folder}')\n",
    "\n",
    "    if not os.path.isdir(f'../results/{folder}/{ID}'):\n",
    "        os.mkdir(f'../results/{folder}/{ID}')\n",
    "\n",
    "    # domain boundaries; the first column is time, the second column is depth (positive upward)\n",
    "    ics_coords_1 = np.array([[0.0, 0.0],\n",
    "                       [0.0, -10.0]])\n",
    "    ics_coords_2 = np.array([[0.0, 10.0],\n",
    "                           [0.0, 0.0]])\n",
    "    lb_coords = np.array([[0.0, -10.0],   # upper boundary condition\n",
    "                           [10.0, -10.0]])\n",
    "    ub_coords = np.array([[0.0, 10.0],   # lower boundary condition\n",
    "                           [10.0, 10.0]])\n",
    "    dom_coords_1 = np.array([[0.0, 0.0],   # t = 0 hour to t = 10 hours\n",
    "                           [10.0, -10.0]])\n",
    "    dom_coords_2 = np.array([[0.0, 10.0],   # t = 0 hour to t = 10 hours\n",
    "                           [10.0, 0.0]])\n",
    "    int_coords = np.array([[0.0, 0.0],\n",
    "                           [10.0, 0.0]])\n",
    "\n",
    "    # import true solution: first index is depth, second index is time\n",
    "    psi_true = np.load(f\"../../analytical_solutions/Srivastava_psi_heterogeneous.npy\")\n",
    "\n",
    "    # import residual points \n",
    "    Z_res_1 = np.load(f\"data/residual_points_layer1.npy\")\n",
    "    Z_res_2 = np.load(f\"data/residual_points_layer2.npy\")\n",
    "    Z_int = np.load(f\"data/residual_points_interface.npy\")\n",
    "\n",
    "    # hydraulic paramters\n",
    "    hydraulic_parameters_1 = Srivastava_parameters[\"layer1\"]\n",
    "    hydraulic_parameters_2 = Srivastava_parameters[\"layer2\"]\n",
    "\n",
    "\n",
    "    # hydralic function model\n",
    "    hydraulic_model = \"Gardner\"\n",
    "\n",
    "    # normalization\n",
    "    normalize = False\n",
    "\n",
    "    # NN architecture\n",
    "    layers_psi_1 = [2] + number_layers_1*[number_units_1] + [1]\n",
    "    layers_psi_2 = [2] + number_layers_2*[number_units_2] + [1]\n",
    "\n",
    "    # forward solution\n",
    "    # collocation points\n",
    "    if RAR:\n",
    "        number_res_1_2 =  number_res_1 - (RAR_iterations - 1) * 10\n",
    "        number_res_2_2 =  number_res_2 - (RAR_iterations - 1) * 10\n",
    "    else:\n",
    "        number_res_1_2 = number_res_1\n",
    "        number_res_2_2 = number_res_2\n",
    "\n",
    "\n",
    "    t_res_1 = Z_res_1[:number_res_1_2, 0:1]\n",
    "    z_res_1 = Z_res_1[:number_res_1_2, 1:2]\n",
    "\n",
    "    t_res_2 = Z_res_2[:number_res_2_2, 0:1]\n",
    "    z_res_2 = Z_res_2[:number_res_2_2, 1:2]\n",
    "\n",
    "    t_res = np.concatenate((t_res_2, t_res_1), axis = 1)\n",
    "    z_res = np.concatenate((z_res_2, z_res_1), axis = 1) \n",
    "\n",
    "    t_int = Z_int[:number_int, 0:1]\n",
    "    z_int = Z_int[:number_int, 1:2]\n",
    "\n",
    "    # initial points\n",
    "\n",
    "    t_ic_1 = np.linspace(ics_coords_1[0, 0], ics_coords_1[1, 0], 101)[:, None]\n",
    "    z_ic_1 = np.linspace(ics_coords_1[0, 1], ics_coords_1[1, 1], 101)[:, None]\n",
    "\n",
    "    t_ic_2 = np.linspace(ics_coords_2[0, 0], ics_coords_2[1, 0], 101)[:, None]\n",
    "    z_ic_2 = np.linspace(ics_coords_2[0, 1], ics_coords_2[1, 1], 101)[:, None]\n",
    "\n",
    "\n",
    "    # upper boundary points\n",
    "\n",
    "    t_ub = np.linspace(ub_coords[0, 0], ub_coords[1, 0], number_ub + 1)[1:, None]\n",
    "    z_ub = np.linspace(ub_coords[0, 1], ub_coords[1, 1], number_ub + 1)[1:, None]\n",
    "\n",
    "    # lower boundary points\n",
    "\n",
    "    t_lb = np.linspace(lb_coords[0, 0], lb_coords[1, 0], 101)[:, None]\n",
    "    z_lb = np.linspace(lb_coords[0, 1], lb_coords[1, 1], 101)[:, None]\n",
    "\n",
    "    # initial condition\n",
    "    # upper water flux condition (positive downward) [cm/hour]\n",
    "    q_A1_star = 0.1\n",
    "    q_A2_star = 0.1\n",
    "    q_B2_star = 0.9\n",
    "\n",
    "    # parameters\n",
    "    alpha1 = 1.0\n",
    "    alpha2 = 1.0\n",
    "    K_s1 = 1.0\n",
    "    K_s2 = 10.0\n",
    "    theta_s1 = 0.40\n",
    "    theta_s2 = 0.40\n",
    "    theta_r1 = 0.06\n",
    "    theta_r2 = 0.06\n",
    "    L1_star = 10.0\n",
    "    L2_star = 10.0\n",
    "\n",
    "    L1 = L1_star * alpha1\n",
    "    L2 = L2_star * alpha2\n",
    "\n",
    "    # pressure at the lower boundary (water table)\n",
    "    psi_0 = 0.0\n",
    "\n",
    "    q_A1 = q_A1_star / K_s1\n",
    "\n",
    "    q_A2 = q_A2_star / K_s2\n",
    "\n",
    "    q_B2 = q_B2_star / K_s2\n",
    "\n",
    "    # initial condition\n",
    "    K_ic_1 = q_A1 - (q_A1 - np.exp(alpha1 * psi_0)) * np.exp(-(L1 + z_ic_1 * alpha1))\n",
    "    psi_ic_1 = np.log(K_ic_1) /alpha1\n",
    "\n",
    "    K_ic_2 = q_A2 - (q_A2 - (q_A1 - (q_A1 - np.exp(alpha1 * psi_0)) \\\n",
    "           *np.exp(-L1))**(alpha2/alpha1))* np.exp(-z_ic_2 * alpha2)\n",
    "    psi_ic_2 = np.log(K_ic_2) /alpha2\n",
    "\n",
    "    theta_ic_1 = theta_r1 + (theta_s1 - theta_r1) * np.exp(alpha1 * psi_ic_1)\n",
    "    theta_ic_2 = theta_r2 + (theta_s2 - theta_r2) * np.exp(alpha2 * psi_ic_2)\n",
    "\n",
    "\n",
    "    # boundary conditions\n",
    "    theta_lb = theta_r1 + (theta_s1 - theta_r1) * np.exp(alpha1 * psi_0)\n",
    "    qz_ub = -q_B2_star * np.ones(t_ub.shape)\n",
    "    theta_lb = theta_lb * np.ones(t_lb.shape)\n",
    "\n",
    "    # initialization\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(random_seed)\n",
    "    random.seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "\n",
    "    # training    \n",
    "    model = PINNs_RRE1D(\n",
    "                t_res_1, z_res_1,\n",
    "                t_res_2, z_res_2,\n",
    "                t_int, z_int,\n",
    "                t_ic_1, z_ic_1, theta_ic_1,\n",
    "                t_ic_2, z_ic_2, theta_ic_2,\n",
    "                t_ub, z_ub, qz_ub,\n",
    "                t_lb, z_lb, theta_lb,      \n",
    "                layers_psi_1, layers_psi_2, hydraulic_model, hydraulic_parameters_1, hydraulic_parameters_2,\n",
    "                weight_values, normalize, ALR, LAA\n",
    "                   )\n",
    "\n",
    "    # Adam training\n",
    "    start_time = timeit.default_timer() \n",
    "\n",
    "    if RAR:\n",
    "        iterations = iterations//10\n",
    "        model.RAR(dom_coords, iterations, batch_size, new_residual_points)\n",
    "    else:\n",
    "        model.train_Adam(iterations, batch_size)\n",
    "    time_Adam = timeit.default_timer() - start_time\n",
    "\n",
    "    # BFGS training\n",
    "    start_time = timeit.default_timer() \n",
    "    model.train_BFGS()\n",
    "    time_BFGS = timeit.default_timer() - start_time\n",
    "\n",
    "    # evaluation\n",
    "    T = 101\n",
    "    N = 101\n",
    "    t_1 = np.linspace(dom_coords_1[0, 0], dom_coords_1[1, 0], T)[:, None]\n",
    "    z_1 = np.linspace(dom_coords_1[0, 1], dom_coords_1[1, 1], N)[:, None]\n",
    "    z_1_mesh, t_1_mesh = np.meshgrid(z_1, t_1)\n",
    "    X_star_1 = np.hstack((t_1_mesh.flatten()[:, None], z_1_mesh.flatten()[:, None]))\n",
    "\n",
    "    t_2 = np.linspace(dom_coords_2[0, 0], dom_coords_2[1, 0], T)[:, None]\n",
    "    z_2 = np.linspace(dom_coords_2[0, 1], dom_coords_2[1, 1], N)[:, None]\n",
    "    z_2_mesh, t_2_mesh = np.meshgrid(z_2, t_2)\n",
    "    X_star_2 = np.hstack((t_2_mesh.flatten()[:, None], z_2_mesh.flatten()[:, None]))\n",
    "\n",
    "    t_test_1 = X_star_1[:, 0:1]\n",
    "    z_test_1 = X_star_1[:, 1:2]\n",
    "\n",
    "    t_test_2 = X_star_2[:, 0:1]\n",
    "    z_test_2 = X_star_2[:, 1:2]\n",
    "\n",
    "    # prediction\n",
    "    psi_1_pred, theta_1_pred, K_1_pred, flux_1_pred, residual_1 = model.predict_1(t_test_1, z_test_1)\n",
    "    psi_2_pred, theta_2_pred, K_2_pred, flux_2_pred, residual_2 = model.predict_2(t_test_2, z_test_2)\n",
    "\n",
    "    theta_1_pred = theta_1_pred.reshape((T, N))\n",
    "    K_1_pred = K_1_pred.reshape((T, N))\n",
    "    psi_1_pred = psi_1_pred.reshape((T, N))\n",
    "    flux_1_pred = flux_1_pred.reshape((T, N))\n",
    "    residual_1 = residual_1.reshape((T, N))\n",
    "\n",
    "\n",
    "    theta_2_pred = theta_2_pred.reshape((T, N))\n",
    "    K_2_pred = K_2_pred.reshape((T, N))\n",
    "    psi_2_pred = psi_2_pred.reshape((T, N))\n",
    "    flux_2_pred = flux_2_pred.reshape((T, N))\n",
    "    residual_2 = residual_2.reshape((T, N))\n",
    "\n",
    "    psi_pred = np.concatenate((psi_2_pred, psi_1_pred), axis = 1) \n",
    "    theta_pred = np.concatenate((theta_2_pred, theta_1_pred), axis = 1)\n",
    "    K_pred = np.concatenate((K_2_pred, K_1_pred), axis = 1)\n",
    "    residual = np.concatenate((residual_2, residual_1), axis = 1)\n",
    "    flux_pred = np.concatenate((flux_2_pred, flux_1_pred), axis = 1) \n",
    "\n",
    "    t_mesh = np.concatenate((t_2_mesh, t_1_mesh), axis = 1)\n",
    "    z_mesh = np.concatenate((z_2_mesh, z_1_mesh), axis = 1) \n",
    "\n",
    "    theta_1_true = theta_r1 + (theta_s1 - theta_r1) * np.exp(alpha1 * psi_true[101:,:])\n",
    "    theta_2_true = theta_r2 + (theta_s2 - theta_r2) * np.exp(alpha2 * psi_true[0:101,:])\n",
    "\n",
    "    theta_true = np.concatenate((theta_2_true, theta_1_true), axis = 0)\n",
    "\n",
    "    # the error at the interface is computed \"twice\" (theta in layer 1 and layer 2, which are not necesarily the same)\n",
    "    theta_L2_error = np.sqrt(np.mean((theta_true.T - theta_pred) ** 2))\n",
    "    theta_abs_error = np.mean(np.abs(theta_true.T- theta_pred))\n",
    "    theta_square = np.sqrt(np.mean((theta_true.T) ** 2))\n",
    "    theta_max_error = np.max(np.abs(theta_true.T- theta_pred))\n",
    "\n",
    "    theta_L2_relative_error = theta_L2_error/theta_square\n",
    "    theta_abs_relative_error = theta_abs_error/np.mean(np.abs(theta_true.T))\n",
    "\n",
    "    print(\"theta_L2_relative_error is\", theta_L2_relative_error)\n",
    "    print(\"theta_abs_relative_error is\", theta_abs_relative_error)\n",
    "    print(\"theta_max_error is\", theta_max_error)\n",
    "\n",
    "    psi_L2_error = np.sqrt(np.mean((psi_true.T - psi_pred) ** 2))\n",
    "    psi_abs_error = np.mean(np.abs(psi_true.T- psi_pred))\n",
    "    psi_square = np.sqrt(np.mean((psi_true.T) ** 2))\n",
    "    psi_max_error = np.max(np.abs(psi_true.T- psi_pred))\n",
    "\n",
    "    psi_L2_relative_error = psi_L2_error/psi_square\n",
    "    psi_abs_relative_error = psi_abs_error/np.mean(np.abs(psi_true.T))\n",
    "\n",
    "\n",
    "    print(\"psi_L2_relative_error is\", psi_L2_relative_error)\n",
    "    print(\"psi_abs_relative_error is\", psi_abs_relative_error)\n",
    "    print(\"psi_max_error is\", psi_max_error)\n",
    "\n",
    "    # theta distribution\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "    c1 = ax[0].pcolor(t_mesh, z_mesh - 10.0, theta_pred, cmap='gnuplot2_r')\n",
    "    fig.colorbar(c1, ax = ax[0])\n",
    "    ax[0].set_xlabel(r'$t$ [hours]')\n",
    "    ax[0].set_ylabel(r'$z$ [cm]')\n",
    "    ax[0].set_title('Predicted $\\\\theta$')\n",
    "\n",
    "    c2 = ax[1].pcolor(t_mesh, z_mesh - 10.0, theta_true.T, cmap='gnuplot2_r')\n",
    "    fig.colorbar(c2, ax = ax[1])\n",
    "    ax[1].set_xlabel(r'$t$ [hours]')\n",
    "    ax[1].set_title('True $\\\\theta$')\n",
    "\n",
    "    c3 = ax[2].pcolor(t_mesh, z_mesh - 10.0, np.abs(theta_pred - theta_true.T), cmap='gnuplot2_r')\n",
    "    fig.colorbar(c3, ax = ax[2])\n",
    "    ax[2].set_xlabel(r'$t$ [hours]')\n",
    "    ax[2].set_title('Absolute error in $\\\\theta$')\n",
    "\n",
    "    fig.savefig(f'../results/{folder}/{ID}/theta_color.png')\n",
    "    fig.clf()\n",
    "\n",
    "    # psi distribution\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "    c1 = ax[0].pcolor(t_mesh, z_mesh - 10.0, psi_pred, cmap='gnuplot2_r')\n",
    "    fig.colorbar(c1, ax = ax[0])\n",
    "    ax[0].set_xlabel(r'$t$ [hours]')\n",
    "    ax[0].set_ylabel(r'$z$ [cm]')\n",
    "    ax[0].set_title('Predicted $\\\\psi$ [cm]')\n",
    "\n",
    "    c2 = ax[1].pcolor(t_mesh, z_mesh - 10.0, psi_true.T, cmap='gnuplot2_r')\n",
    "    fig.colorbar(c2, ax = ax[1])\n",
    "    ax[1].set_xlabel(r'$t$ [hours]')\n",
    "    ax[1].set_title('True $\\\\psi$ [cm]')\n",
    "\n",
    "    c3 = ax[2].pcolor(t_mesh, z_mesh - 10.0, np.abs(psi_pred - psi_true.T), cmap='gnuplot2_r')\n",
    "    fig.colorbar(c3, ax = ax[2])\n",
    "    ax[2].set_xlabel(r'$t$ [hours]')\n",
    "    ax[2].set_title('Absolute error in $\\\\psi$ [cm]')\n",
    "\n",
    "    fig.savefig(f'../results/{folder}/{ID}/psi_color.png')\n",
    "    fig.clf()\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(6.5, 6.5))\n",
    "\n",
    "    c1 = ax.pcolor(t_mesh, z_mesh - 10.0, flux_pred, cmap='gnuplot2_r')\n",
    "    fig.colorbar(c1, ax = ax)\n",
    "    ax.set_xlabel(r'$t$ [hours]')\n",
    "    ax.set_ylabel(r'$z$ [cm]')\n",
    "    ax.set_title('Predicted $q$ [cm/hours]')\n",
    "\n",
    "    fig.savefig(f'../results/{folder}/{ID}/flux.png')\n",
    "    fig.clf()\n",
    "\n",
    "    fig = plt.subplots(1, 1, figsize=(6.5, 6.5))\n",
    "\n",
    "    plt.scatter(theta_pred[0, ::5], z_mesh[0, ::5] - 10.0, s = 15, label = 'Predicted')\n",
    "    plt.scatter(theta_pred[1, ::5], z_mesh[0, ::5] - 10.0, s = 15)\n",
    "    plt.scatter(theta_pred[5, ::5], z_mesh[0, ::5] - 10.0, s = 15)\n",
    "    plt.scatter(theta_pred[10, ::5], z_mesh[0, ::5] - 10.0, s = 15)\n",
    "    plt.scatter(theta_pred[30, ::5], z_mesh[0, ::5] - 10.0, s = 15)\n",
    "    plt.scatter(theta_pred[50, ::5], z_mesh[0, ::5] - 10.0, s = 15)\n",
    "    plt.scatter(theta_pred[100, ::5], z_mesh[0, ::5] - 10.0, s = 15)\n",
    "\n",
    "    plt.plot(theta_true.T[0, :], z_mesh[0, :] - 10.0 ,label = 't = 0 hour')\n",
    "    plt.plot(theta_true.T[1, :], z_mesh[0, :] - 10.0,label = 't = 0.1 hour')\n",
    "    plt.plot(theta_true.T[5, :], z_mesh[0, :] - 10.0,label = 't = 0.5 hour')\n",
    "    plt.plot(theta_true.T[10, :], z_mesh[0, :]- 10.0 ,label = 't = 1 hour')\n",
    "    plt.plot(theta_true.T[30, :], z_mesh[0, :]- 10.0,label = 't = 3 hour')\n",
    "    plt.plot(theta_true.T[50, :], z_mesh[0, :]- 10.0,label = 't = 5 hour')\n",
    "    plt.plot(theta_true.T[100, :], z_mesh[0, :]- 10.0, label = 't = 10 hour')\n",
    "\n",
    "    plt.xlim(0.05,0.50)\n",
    "    plt.xlabel('$\\\\theta$')\n",
    "    plt.ylabel('$z$ cm')\n",
    "    plt.legend(loc = (0.75,0.55), fontsize = 8)\n",
    "\n",
    "    plt.savefig(f'../results/{folder}/{ID}/theta_fixed_time.png')\n",
    "    plt.clf()\n",
    "\n",
    "    t_res = np.concatenate((t_res_2, t_res_1), axis = 1)\n",
    "    z_res = np.concatenate((z_res_2, z_res_1), axis = 1) \n",
    "\n",
    "    fig = plt.figure(1, figsize=(6.5, 6.5))\n",
    "\n",
    "    plt.plot(t_res, z_res - 10.0, 'rx', markersize = 3, label = \"residual points\")\n",
    "\n",
    "    if RAR:\n",
    "        new_residual = model.new_residual[0]\n",
    "        for i in range(1, len(model.new_residual)):\n",
    "            new_residual = np.vstack((new_residual, model.new_residual[i]))\n",
    "        plt.plot(new_residual[:, 0], new_residual[:, 1] - 10.0, 'bx', markersize = 5, label = \"added residual points\")\n",
    "\n",
    "\n",
    "\n",
    "    plt.xlabel(r'$t$ [hours]')\n",
    "    plt.ylabel(r'$z$ [cm]')\n",
    "    plt.title('residual points')\n",
    "    plt.xlim(-0.1, 10.1)\n",
    "    plt.ylim(-10.1, 0.1)\n",
    "    plt.legend(loc = (0.55,0.15))\n",
    "    plt.savefig(f'../results/{folder}/{ID}/residual_points.png', transparent = True)\n",
    "    plt.clf()\n",
    "\n",
    "    fig = plt.figure(1, figsize=(6.5, 6.5))\n",
    "\n",
    "    plt.pcolor(t_mesh, z_mesh - 10.0, np.log10(np.abs(residual)), cmap='gnuplot2_r')\n",
    "\n",
    "    plt.colorbar()\n",
    "    plt.xlabel(r'$t$ [hours]')\n",
    "    plt.ylabel(r'$z$ [cm]')\n",
    "    plt.title('$log_{10}r(x)$')\n",
    "    plt.savefig(f'../results/{folder}/{ID}/residual.png', transparent = True)\n",
    "    plt.clf()\n",
    "\n",
    "    # Loss\n",
    "\n",
    "    loss_res_1 = model.loss_res_1_log\n",
    "    loss_ic_1 = model.loss_ic_1_log\n",
    "    loss_lb = model.loss_lb_log\n",
    "    loss_res_2 = model.loss_res_2_log\n",
    "    loss_ic_2 = model.loss_ic_2_log\n",
    "    loss_ub = model.loss_ub_log\n",
    "    loss_psi_int = model.loss_psi_int_log\n",
    "    loss_qz_int = model.loss_qz_int_log\n",
    "    loss_res_int = model.loss_res_int_log\n",
    "\n",
    "    Adam_iterations = np.arange(0, len(loss_res_1)) * 10\n",
    "\n",
    "    loss_res_1_BFGS = model.loss_res_1_log_BFGS\n",
    "    loss_ic_1_BFGS = model.loss_ic_1_log_BFGS\n",
    "    loss_lb_BFGS = model.loss_lb_log_BFGS\n",
    "    loss_res_2_BFGS = model.loss_res_2_log_BFGS\n",
    "    loss_ic_2_BFGS = model.loss_ic_2_log_BFGS\n",
    "    loss_ub_BFGS = model.loss_ub_log_BFGS\n",
    "    loss_psi_int_BFGS = model.loss_psi_int_log_BFGS\n",
    "    loss_qz_int_BFGS = model.loss_qz_int_log_BFGS\n",
    "    loss_res_int_BFGS = model.loss_res_int_log_BFGS\n",
    "\n",
    "    BFGS_iterations = np.arange(0, len(loss_res_1_BFGS))\n",
    "\n",
    "    total_iterations = np.append(Adam_iterations, BFGS_iterations + Adam_iterations[-1])\n",
    "\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "    # Adam\n",
    "    ax[0].plot(Adam_iterations, loss_ic_1, label='$\\mathcal{L}_{ic1}$')\n",
    "    ax[0].plot(Adam_iterations, loss_ic_2, label='$\\mathcal{L}_{ic2}$')\n",
    "    ax[0].plot(Adam_iterations, loss_ub ,label='$\\mathcal{L}_{ub}$')\n",
    "    ax[0].plot(Adam_iterations, loss_lb,label='$\\mathcal{L}_{lb}$')\n",
    "    ax[0].plot(Adam_iterations, loss_res_1, label='$\\mathcal{L}_{res1}$')\n",
    "    ax[0].plot(Adam_iterations, loss_res_2, label='$\\mathcal{L}_{res2}$')\n",
    "    ax[0].plot(Adam_iterations, loss_psi_int, label='$\\mathcal{L}_{\\psi-int}$')\n",
    "    ax[0].plot(Adam_iterations, loss_qz_int, label='$\\mathcal{L}_{qz-int}$')\n",
    "    ax[0].plot(Adam_iterations, loss_res_int, label='$\\mathcal{L}_{res-int}$')\n",
    "    ax[0].set_yscale('log')\n",
    "    ax[0].set_xlabel('iterations')\n",
    "    ax[0].set_ylabel('Loss')\n",
    "    ax[0].set_title('Adam')\n",
    "    ax[0].legend()\n",
    "\n",
    "    # BFGS\n",
    "    ax[1].plot(BFGS_iterations, loss_ic_1_BFGS, label='$\\mathcal{L}_{ic1}$')\n",
    "    ax[1].plot(BFGS_iterations, loss_ic_2_BFGS, label='$\\mathcal{L}_{ic2}$')\n",
    "    ax[1].plot(BFGS_iterations, loss_ub_BFGS ,label='$\\mathcal{L}_{ub}$')\n",
    "    ax[1].plot(BFGS_iterations, loss_lb_BFGS,label='$\\mathcal{L}_{lb}$')\n",
    "    ax[1].plot(BFGS_iterations, loss_res_1_BFGS, label='$\\mathcal{L}_{res1}$')\n",
    "    ax[1].plot(BFGS_iterations, loss_res_2_BFGS, label='$\\mathcal{L}_{res2}$')\n",
    "    ax[1].plot(BFGS_iterations, loss_psi_int_BFGS, label='$\\mathcal{L}_{\\psi-int}$')\n",
    "    ax[1].plot(BFGS_iterations, loss_qz_int_BFGS, label='$\\mathcal{L}_{qz-int}$')\n",
    "    ax[1].plot(BFGS_iterations, loss_res_int_BFGS, label='$\\mathcal{L}_{res-int}$')\n",
    "\n",
    "    ax[1].set_yscale('log')\n",
    "    ax[1].set_xlabel('iterations')\n",
    "    ax[1].set_ylabel('Loss')\n",
    "    ax[1].set_title('L-BFGS-B')\n",
    "    ax[1].legend()\n",
    "\n",
    "\n",
    "    # total training\n",
    "    ax[2].plot(total_iterations, loss_ic_1 + loss_ic_1_BFGS, label='$\\mathcal{L}_{ic1}$')\n",
    "    ax[2].plot(total_iterations, loss_ic_2 + loss_ic_2_BFGS, label='$\\mathcal{L}_{ic2}$')\n",
    "    ax[2].plot(total_iterations, loss_ub + loss_ub_BFGS ,label='$\\mathcal{L}_{ub}$')\n",
    "    ax[2].plot(total_iterations, loss_lb + loss_lb_BFGS,label='$\\mathcal{L}_{lb}$')\n",
    "    ax[2].plot(total_iterations, loss_res_1 + loss_res_1_BFGS, label='$\\mathcal{L}_{res1}$')\n",
    "    ax[2].plot(total_iterations, loss_res_2 + loss_res_2_BFGS, label='$\\mathcal{L}_{res2}$')\n",
    "    ax[2].plot(total_iterations, loss_psi_int + loss_psi_int_BFGS, label='$\\mathcal{L}_{\\psi-int}$')\n",
    "    ax[2].plot(total_iterations, loss_qz_int + loss_qz_int_BFGS, label='$\\mathcal{L}_{qz-int}$')\n",
    "    ax[2].plot(total_iterations, loss_res_int + loss_res_int_BFGS, label='$\\mathcal{L}_{res-int}$')\n",
    "\n",
    "\n",
    "    ax[2].set_yscale('log')\n",
    "    ax[2].set_xlabel('iterations')\n",
    "    ax[2].set_ylabel('Loss')\n",
    "    ax[2].set_title('total')\n",
    "    ax[2].legend()\n",
    "\n",
    "    plt.savefig(f'../results/{folder}/{ID}/training.png')\n",
    "    plt.clf()\n",
    "\n",
    "  \n",
    "    # store data\n",
    "\n",
    "    np.save(f\"../results/{folder}/{ID}/theta_pred\", theta_pred)\n",
    "    np.save(f\"../results/{folder}/{ID}/psi_pred\", psi_pred)\n",
    "    np.save(f\"../results/{folder}/{ID}/K_pred\", K_pred)\n",
    "    np.save(f\"../results/{folder}/{ID}/flux_pred\", flux_pred)\n",
    "    np.save(f\"../results/{folder}/{ID}/residual\", residual)\n",
    "\n",
    "    np.save(f\"../results/{folder}/{ID}/loss_ic_1\", loss_ic_1)\n",
    "    np.save(f\"../results/{folder}/{ID}/loss_ic_1_BFGS\", loss_ic_1_BFGS)\n",
    "    np.save(f\"../results/{folder}/{ID}/loss_ic_2\", loss_ic_1)\n",
    "    np.save(f\"../results/{folder}/{ID}/loss_ic_2_BFGS\", loss_ic_2_BFGS)\n",
    "\n",
    "    np.save(f\"../results/{folder}/{ID}/loss_ub\", loss_ub)\n",
    "    np.save(f\"../results/{folder}/{ID}/loss_ub_BFGS\", loss_ub_BFGS)\n",
    "    np.save(f\"../results/{folder}/{ID}/loss_lb\", loss_lb)\n",
    "    np.save(f\"../results/{folder}/{ID}/loss_lb_BFGS\", loss_lb_BFGS)\n",
    "\n",
    "    np.save(f\"../results/{folder}/{ID}/loss_res_1\", loss_res_1)\n",
    "    np.save(f\"../results/{folder}/{ID}/loss_res_1_BFGS\", loss_res_1_BFGS)\n",
    "    np.save(f\"../results/{folder}/{ID}/loss_res_2\", loss_res_2)\n",
    "    np.save(f\"../results/{folder}/{ID}/loss_res_2_BFGS\", loss_res_2_BFGS)\n",
    "\n",
    "    np.save(f\"../results/{folder}/{ID}/loss_psi_int\", loss_psi_int)\n",
    "    np.save(f\"../results/{folder}/{ID}/loss_psi_int_BFGS\", loss_psi_int_BFGS)\n",
    "    np.save(f\"../results/{folder}/{ID}/loss_qz_int\", loss_qz_int)\n",
    "    np.save(f\"../results/{folder}/{ID}/loss_qz_int_BFGS\", loss_qz_int_BFGS)\n",
    "    np.save(f\"../results/{folder}/{ID}/loss_res_int\", loss_res_int)\n",
    "    np.save(f\"../results/{folder}/{ID}/loss_res_int_BFGS\", loss_res_int_BFGS)\n",
    "\n",
    "\n",
    "    np.save(f\"../results/{folder}/{ID}/t_res\", t_res)\n",
    "    np.save(f\"../results/{folder}/{ID}/z_res\", z_res)\n",
    "    if RAR:\n",
    "        np.save(f\"../results/{folder}/{ID}/new_residual\", new_residual)\n",
    "\n",
    "    np.save(f\"../results/{folder}/{ID}/z_mesh\", z_mesh)\n",
    "    np.save(f\"../results/{folder}/{ID}/t_mesh\", t_mesh)\n",
    "\n",
    "\n",
    "    sheet1.write(ID, 0, ID)\n",
    "    sheet1.write(ID, 1, random_seed)\n",
    "    sheet1.write(ID, 2, number_layers_1)\n",
    "    sheet1.write(ID, 3, number_units_1)\n",
    "    sheet1.write(ID, 4, number_layers_2)\n",
    "    sheet1.write(ID, 5, number_units_2)\n",
    "    sheet1.write(ID, 6, LAA)\n",
    "    sheet1.write(ID, 7, number_res_1)\n",
    "    sheet1.write(ID, 8, number_res_2)\n",
    "    sheet1.write(ID, 9, number_ub)\n",
    "    sheet1.write(ID, 10, number_int)\n",
    "\n",
    "    sheet1.write(ID, 11, weight_values[0])\n",
    "    sheet1.write(ID, 12, weight_values[1])\n",
    "    sheet1.write(ID, 13, weight_values[2])\n",
    "    sheet1.write(ID, 14, weight_values[3])\n",
    "    sheet1.write(ID, 15, weight_values[4])\n",
    "    sheet1.write(ID, 16, weight_values[5])\n",
    "    sheet1.write(ID, 17, weight_values[6])\n",
    "    sheet1.write(ID, 18, weight_values[7])\n",
    "\n",
    "    sheet1.write(ID, 19, ALR)\n",
    "    sheet1.write(ID, 20, iterations)\n",
    "    sheet1.write(ID, 21, batch_size)\n",
    "    sheet1.write(ID, 22, RAR)\n",
    "    sheet1.write(ID, 23, new_residual_points)\n",
    "    sheet1.write(ID, 24, time_Adam)\n",
    "    sheet1.write(ID, 25, time_BFGS)\n",
    "\n",
    "    sheet1.write(ID, 26, theta_L2_relative_error)\n",
    "    sheet1.write(ID, 27, theta_abs_relative_error)\n",
    "    sheet1.write(ID, 28, theta_max_error)\n",
    "    sheet1.write(ID, 29, psi_L2_relative_error)\n",
    "    sheet1.write(ID, 30, psi_abs_relative_error)\n",
    "    sheet1.write(ID, 31, psi_max_error)\n",
    "    sheet1.write(ID, 32, str(loss_ub_BFGS[-1]))\n",
    "    sheet1.write(ID, 33, str(loss_lb_BFGS[-1]))\n",
    "    sheet1.write(ID, 34, str(loss_ic_1_BFGS[-1]))\n",
    "    sheet1.write(ID, 35, str(loss_ic_2_BFGS[-1]))\n",
    "    sheet1.write(ID, 36, str(loss_res_1_BFGS[-1]))\n",
    "    sheet1.write(ID, 37, str(loss_res_2_BFGS[-1]))\n",
    "    sheet1.write(ID, 38, str(loss_res_int_BFGS[-1]))\n",
    "    sheet1.write(ID, 39, str(loss_psi_int_BFGS[-1]))\n",
    "    sheet1.write(ID, 40, str(loss_qz_int_BFGS[-1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"interface\"\n",
    "wb = xlwt.Workbook()\n",
    "sheet1 = wb.add_sheet(f'{folder}')\n",
    "sheet1.write(0, 0,\"ID\")\n",
    "sheet1.write(0, 1, \"random_seed\")\n",
    "sheet1.write(0, 2, \"number_layers_1\")\n",
    "sheet1.write(0, 3, \"number_units_1\")\n",
    "sheet1.write(0, 4, \"number_layers_2\")\n",
    "sheet1.write(0, 5, \"number_units_2\")\n",
    "sheet1.write(0, 6, \"LAA\")\n",
    "sheet1.write(0, 7, \"number_res_1\")\n",
    "sheet1.write(0, 8, \"number_res_2\")\n",
    "sheet1.write(0, 9, \"number_ub\")\n",
    "sheet1.write(0, 10, \"number_int\")\n",
    "sheet1.write(0, 11, \"weight_values_ic_1\")\n",
    "sheet1.write(0, 12, \"weight_values_lb\")\n",
    "sheet1.write(0, 13, \"weight_values_res_2\")\n",
    "sheet1.write(0, 14, \"weight_values_ic_2\")\n",
    "sheet1.write(0, 15, \"weight_values_ub\")\n",
    "sheet1.write(0, 16, \"weight_values_int\") # this should have been res_int\n",
    "sheet1.write(0, 17, \"weight_values_psi_int\")\n",
    "sheet1.write(0, 18, \"weight_values_qz_int\")\n",
    "sheet1.write(0, 19, \"ALR\")\n",
    "sheet1.write(0, 20, \"iterations\")\n",
    "sheet1.write(0, 21, \"batch_size\")\n",
    "sheet1.write(0, 22, \"RAR\")\n",
    "sheet1.write(0, 23, \"new_residual_points\")\n",
    "sheet1.write(0, 24, \"time_Adam\")\n",
    "sheet1.write(0, 25, \"time_BFGS\")\n",
    "sheet1.write(0, 26, \"theta_L2_relative_error\")\n",
    "sheet1.write(0, 27, \"theta_abs_relative_error\")\n",
    "sheet1.write(0, 28, \"theta_max_error\")\n",
    "sheet1.write(0, 29, \"psi_L2_relative_error\")\n",
    "sheet1.write(0, 30, \"psi_abs_relative_error\")\n",
    "sheet1.write(0, 31, \"psi_max_error\")\n",
    "sheet1.write(0, 32,  \"loss_ub\")\n",
    "sheet1.write(0, 33,  \"loss_lb\")\n",
    "sheet1.write(0, 34,  \"loss_ic_1\")\n",
    "sheet1.write(0, 35,  \"loss_ic_2\")\n",
    "sheet1.write(0, 36,  \"loss_res_1\")\n",
    "sheet1.write(0, 37,  \"loss_res_2\")\n",
    "sheet1.write(0, 38,  \"loss_res_int\")\n",
    "sheet1.write(0, 39,  \"loss_psi_int\")\n",
    "sheet1.write(0, 40,  \"loss_qz_int\")\n",
    "\n",
    "random_seed_list = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "number_int_list = [100, 300, 1000, 3000, 10000]\n",
    "  \n",
    "ID = 1\n",
    "\n",
    "for i in range(len(number_int_list)):\n",
    "    for j in range(len(random_seed_list)):\n",
    "        main_loop(folder, ID, random_seed_list[j], number_int_list[i])\n",
    "        ID += 1\n",
    "        wb.save(f'../results/{folder}/summary.xls')"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "UFnets_Srivastva_case3_final4.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
