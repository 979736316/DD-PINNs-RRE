{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KQpI7h0VNFxj",
    "outputId": "1996f9cb-be6f-484d-abd3-b6abd3f60ec9"
   },
   "outputs": [],
   "source": [
    "# %tensorflow_version 2.x\n",
    "# !pip uninstall -y tensorflow\n",
    "# !pip install tensorflow-gpu==1.15.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Os1YQiUmNBUH"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import timeit\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import griddata\n",
    "import random\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from glob import glob \n",
    "import xlwt\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2qclbFWvNBUJ"
   },
   "outputs": [],
   "source": [
    "class neural_net(object):\n",
    "    def __init__(self, *inputs, layers, monotonic = False, normalize = False, LAA = False):\n",
    "\n",
    "        self.layers = layers\n",
    "        self.num_layers = len(self.layers)\n",
    "        self.monotonic = monotonic\n",
    "        self.normalize = normalize\n",
    "        self.LAA = LAA\n",
    "        \n",
    "        if self.normalize:\n",
    "            if len(inputs) == 0:\n",
    "                in_dim = self.layers[0]\n",
    "                self.X_mean = np.zeros([1, in_dim])\n",
    "                self.X_std = np.ones([1, in_dim])\n",
    "            else:\n",
    "                X = np.concatenate(inputs, 1)\n",
    "                self.X_mean = X.mean(0, keepdims=True)\n",
    "                self.X_std = X.std(0, keepdims=True)\n",
    "\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        self.A = []\n",
    "\n",
    "        for l in range(0,self.num_layers-1):\n",
    "            in_dim = self.layers[l]\n",
    "            out_dim = self.layers[l+1]\n",
    "            xavier_stddev = np.sqrt(2/(in_dim + out_dim))\n",
    "            W = tf.Variable(tf.random.truncated_normal([in_dim, out_dim], stddev = xavier_stddev),dtype=tf.float32, trainable=True) \n",
    "            # monotonically increasing neural network\n",
    "            if self.monotonic:\n",
    "                W = W**2\n",
    "            b = tf.Variable(np.zeros([1, out_dim]), dtype=tf.float32, trainable=True)\n",
    "            \n",
    "            # tensorflow variables\n",
    "            self.weights.append(W)\n",
    "            self.biases.append(b)\n",
    "            \n",
    "            # locally adaptive activation\n",
    "            if self.LAA:\n",
    "                a = tf.Variable(0.05, dtype=tf.float32)\n",
    "                self.A.append(a)\n",
    "            \n",
    "    def __call__(self, *inputs):\n",
    "        if self.normalize:\n",
    "            H = (tf.concat(inputs, 1) - self.X_mean)/self.X_std\n",
    "        else:\n",
    "            H = tf.concat(inputs, 1)\n",
    "\n",
    "        for l in range(0, self.num_layers-1):\n",
    "            W = self.weights[l]\n",
    "            b = self.biases[l]\n",
    "            \n",
    "            # matrix multiplication\n",
    "            H = tf.matmul(H, W)\n",
    "            # add bias\n",
    "            H = H + b\n",
    "            # activation\n",
    "            if l < self.num_layers-2:\n",
    "                if self.LAA:\n",
    "                    H = tf.tanh(20 * self.A[l]*H)\n",
    "                else:\n",
    "                    H = tf.tanh(H)\n",
    "\n",
    "        return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wRRvIgeuNBUK"
   },
   "outputs": [],
   "source": [
    "def tf_session():\n",
    "    # tf session\n",
    "    config = tf.ConfigProto(allow_soft_placement=True,  # an operation might be placed on CPU for example if GPU is not available\n",
    "                            log_device_placement=True)  # print out device information\n",
    "    config.gpu_options.allow_growth = True\n",
    "    sess = tf.Session(config=config)\n",
    "\n",
    "    # init\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    return sess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kEle8eORNBUK"
   },
   "outputs": [],
   "source": [
    "def VGM(psi, theta_r, theta_s, alpha, n, K_s, l):\n",
    "    \"\"\"\n",
    "    this function calculates volumetric water content and hydraulic conductivity\n",
    "    from matric potential (psi: negative) given VGM (van-Genuchen and Mualem model) parameters\n",
    "    output: (theta, K)\n",
    "    theta: volumetric water content\n",
    "    K: hydraulic conductivity\n",
    "    By default, alpha, n, and K_s are the parameters to be estiated.\n",
    "    \"\"\"\n",
    "    m = 1 - 1/n\n",
    "    S_e = (1 + (-alpha*psi)**n)**(-m)\n",
    "    theta =  S_e * (theta_s - theta_r) + theta_r\n",
    "    K = K_s*S_e**l*(1-(1-S_e**(1/m))**m)**2\n",
    "    return theta, K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_MF78l5mNBUL"
   },
   "outputs": [],
   "source": [
    "# # theta_r, theta_s, alpha [1/cm], n, K_s [cm/hour], l\n",
    "# layer 1 is the lower layer while layer 2 is the upper layer\n",
    "VGM_parameters = {\n",
    "            \"layer2\": [0.078, 0.43, 0.036, 1.56, 1.04, 0.5],\n",
    "            \"layer1\": [0.065, 0.41, 0.075, 1.89, 4.42083, 0.5]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5jmWLJ_sNBUL",
    "outputId": "c12de995-5724-42ff-aef7-c5a339c83390"
   },
   "outputs": [],
   "source": [
    "# sess = tf_session()\n",
    "\n",
    "# log10_h = np.arange(-2, 6, 0.1)  # logarithm of suction (negative matric potential) in cm\n",
    "# psi = -10**log10_h\n",
    "\n",
    "# tf_psi = tf.constant(psi, dtype = tf.float32)\n",
    "\n",
    "# tf_theta_1, tf_K_1 = VGM(tf_psi, *VGM_parameters[\"layer1\"])\n",
    "# tf_theta_2, tf_K_2 = VGM(tf_psi, *VGM_parameters[\"layer2\"])\n",
    "# psi = sess.run(tf_psi)\n",
    "# theta_1 = sess.run(tf_theta_1)\n",
    "# theta_2 = sess.run(tf_theta_2)\n",
    "# K_1 = sess.run(tf_K_1)\n",
    "# K_2 = sess.run(tf_K_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 316
    },
    "id": "HFuMhyt8NBUM",
    "outputId": "01e20f12-2385-45a6-ac4a-21b7c42472b6"
   },
   "outputs": [],
   "source": [
    "# plt.plot(np.log10(-psi), theta_1, label = \"layer1\")\n",
    "# plt.plot(np.log10(-psi), theta_2, label =  \"layer2\")\n",
    "# plt.title(\"Water Retention Curve\")\n",
    "# plt.legend()\n",
    "# plt.xlabel(\"log$_{10}(-\\\\psi)$\")\n",
    "# plt.ylabel(\"$\\\\theta$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 316
    },
    "id": "05jWKQoYNBUM",
    "outputId": "9377db7d-996a-4f73-d2f9-e1c6249ac749"
   },
   "outputs": [],
   "source": [
    "# plt.plot(np.log10(-psi), K_1, label = \"layer 1\")\n",
    "# plt.plot(np.log10(-psi), K_2, label = \"layer 2\")\n",
    "# plt.title(\"Hydraulic Conductivity Function\")\n",
    "# plt.legend()\n",
    "# plt.xlabel(\"log$_{10}(-\\\\psi)$\")\n",
    "# plt.ylabel(\"$K [cm / hour]$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wCf-WqiCNBUN"
   },
   "outputs": [],
   "source": [
    "def log10(x):\n",
    "    numerator = tf.log(x)\n",
    "    denominator = tf.log(tf.constant(10, dtype=numerator.dtype))\n",
    "    return numerator / denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TNbblRDhNBUN"
   },
   "outputs": [],
   "source": [
    "def RRE_mixed_1D(psi, theta, K, t, z):\n",
    "    \n",
    "    theta_t = tf.gradients(theta, t)[0]\n",
    "    theta_z = tf.gradients(theta, z)[0]\n",
    "    \n",
    "    psi_z = tf.gradients(psi, z)[0]\n",
    "    psi_zz = tf.gradients(psi_z, z)[0]\n",
    "    \n",
    "    K_z = tf.gradients(K, z)[0]\n",
    "\n",
    "    qz = -K*(psi_z + 1.0)\n",
    "    residual = theta_t - K_z*psi_z - K*psi_zz - K_z\n",
    "    return residual, qz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zt2cCuq2NBUO"
   },
   "outputs": [],
   "source": [
    "class Sampler:\n",
    "    # Initialize the class\n",
    "    def __init__(self, dim, coords, func, name = None):\n",
    "        self.dim = dim\n",
    "        self.coords = coords\n",
    "        self.func = func\n",
    "        self.name = name\n",
    "    def sample(self, N):\n",
    "        x = self.coords[0:1,:] + (self.coords[1:2,:]-self.coords[0:1,:])*np.random.rand(N, self.dim)\n",
    "        y = self.func(x)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "781Me2ywNBUO"
   },
   "outputs": [],
   "source": [
    "class PINNs_RRE1D(object):\n",
    "    \"\"\"\n",
    "    PINNs_RRE1D class implements physics-informed neural network solver for the inverse modeling of the\n",
    "    Ricahrdson-Ricahrds equation using TensorFlow 1.15.\n",
    "    # notational conventions\n",
    "    _tf: placeholders\n",
    "    _pred: output of neural networks or other parametric models\n",
    "    _res: values related to residual points\n",
    "    _data: training data \n",
    "    _ic: initial condition\n",
    "    _bc: boundary condition (ub: upper; lb: lower for 1D)\n",
    "    _star: prediction (used after training)\n",
    "    _1: lower layer\n",
    "    _2: upper layer\n",
    "    _int: interface of the layers\n",
    "    t: time\n",
    "    z: vertical coordinate\n",
    "    theta: volumetric water content\n",
    "    psi: matric potential\n",
    "    K: hydraulic conductivity\n",
    "    ALR: adaptive learning rate\n",
    "    LAA: layer-wise locally adaptive activation functions\n",
    "    weight_values: [data_1, data_2, res_2, res_int, psi_int, qz_int]\n",
    "    \"\"\"\n",
    "    def __init__(self, t_res_1, z_res_1,\n",
    "                       t_res_2, z_res_2,\n",
    "                       t_int, z_int,\n",
    "                       t_data_1, z_data_1, theta_data_1,\n",
    "                       t_data_2, z_data_2, theta_data_2,\n",
    "                       layers_psi_1, layers_psi_2, hydraulic_model, \n",
    "                       hydraulic_parameters_1, hydraulic_parameters_2, weight_values,\n",
    "                       normalize, ALR, LAA):\n",
    "        \n",
    "        # NN architecture\n",
    "        self.layers_psi_1 = layers_psi_1\n",
    "        self.layers_psi_2 = layers_psi_2\n",
    "        \n",
    "        # Mode \n",
    "        self.hydraulic_model = hydraulic_model  # \"monotonic\", \"VGM\", \"Brooks\",\"PDI\", \"Gardner\"\n",
    "        self.normalize = normalize # normalize input values to neural networks\n",
    "        self.ALR = ALR  # adaptive learning rate\n",
    "        self.LAA = LAA  # locally adaptive activation functions\n",
    "        \n",
    "        # Adaptive re-weighting constant\n",
    "        self.rate = 0.9\n",
    "        \n",
    "        # parameter for the output of neural networks\n",
    "        self.beta = 0.0\n",
    "        \n",
    "        # initial values for weights in the loss function\n",
    "        \n",
    "        self.adaptive_constant_data_1_val = np.array(weight_values[0])\n",
    "        self.adaptive_constant_data_2_val = np.array(weight_values[1])\n",
    "        self.adaptive_constant_res_2_val = np.array(weight_values[2])\n",
    "        self.adaptive_constant_res_int_val = np.array(weight_values[3])\n",
    "        self.adaptive_constant_psi_int_val = np.array(weight_values[4])\n",
    "        self.adaptive_constant_qz_int_val = np.array(weight_values[5])\n",
    "        \n",
    "        # hydraulic parameters  \n",
    "                    \n",
    "        if self.hydraulic_model == \"VGM\":  # theta_r, theta_s, alpha, n, K_s, l\n",
    "            self.theta_r_1 = tf.Variable(hydraulic_parameters_1[0], dtype=tf.float32, trainable=False)\n",
    "            self.theta_s_1 = tf.Variable(hydraulic_parameters_1[1], dtype=tf.float32, trainable=False)\n",
    "            self.alpha_1 = tf.Variable(hydraulic_parameters_1[2], dtype=tf.float32, trainable=False)\n",
    "            self.n_1 = tf.Variable(hydraulic_parameters_1[3], dtype=tf.float32, trainable=False)\n",
    "            self.K_s_1 = tf.Variable(hydraulic_parameters_1[4], dtype=tf.float32, trainable=False)\n",
    "            self.l_1 = tf.Variable(hydraulic_parameters_1[5], dtype=tf.float32, trainable=False)\n",
    "            \n",
    "            self.theta_r_2 = tf.Variable(hydraulic_parameters_2[0], dtype=tf.float32, trainable=False)\n",
    "            self.theta_s_2 = tf.Variable(hydraulic_parameters_2[1], dtype=tf.float32, trainable=False)\n",
    "            self.alpha_2 = tf.Variable(hydraulic_parameters_2[2], dtype=tf.float32, trainable=False)\n",
    "            self.n_2 = tf.Variable(hydraulic_parameters_2[3], dtype=tf.float32, trainable=False)\n",
    "            self.K_s_2 = tf.Variable(hydraulic_parameters_2[4], dtype=tf.float32, trainable=False)\n",
    "            self.l_2 = tf.Variable(hydraulic_parameters_2[5], dtype=tf.float32, trainable=False)\n",
    "\n",
    "        # data\n",
    "        \n",
    "        [self.t_res_1, self.z_res_1] = [t_res_1, z_res_1]\n",
    "        [self.t_res_2, self.z_res_2] = [t_res_2, z_res_2]\n",
    "        [self.t_int, self.z_int] = [t_int, z_int]\n",
    "        [self.t_data_1, self.z_data_1, self.theta_data_1] = [t_data_1, z_data_1, theta_data_1]\n",
    "        [self.t_data_2, self.z_data_2, self.theta_data_2] = [t_data_2, z_data_2, theta_data_2]\n",
    "        \n",
    "        # placeholders\n",
    "        \n",
    "        [self.t_res_1_tf, self.z_res_1_tf] = [tf.placeholder(tf.float32, shape=[None, 1]) for _ in range(2)]\n",
    "        [self.t_res_2_tf, self.z_res_2_tf] = [tf.placeholder(tf.float32, shape=[None, 1]) for _ in range(2)]\n",
    "        [self.t_int_tf, self.z_int_tf] = [tf.placeholder(tf.float32, shape=[None, 1]) for _ in range(2)]\n",
    "\n",
    "        [self.t_data_1_tf, self.z_data_1_tf, self.theta_data_1_tf] = [tf.placeholder(tf.float32, shape=[None, 1]) for _ in range(3)]\n",
    "        [self.t_data_2_tf, self.z_data_2_tf, self.theta_data_2_tf] = [tf.placeholder(tf.float32, shape=[None, 1]) for _ in range(3)]\n",
    "                \n",
    "        # weight parameters in the loss function\n",
    "        self.adaptive_constant_data_1_tf = tf.placeholder(tf.float32, shape=self.adaptive_constant_data_1_val.shape)\n",
    "        self.adaptive_constant_data_2_tf = tf.placeholder(tf.float32, shape=self.adaptive_constant_data_2_val.shape)\n",
    "        self.adaptive_constant_res_2_tf = tf.placeholder(tf.float32, shape=self.adaptive_constant_res_2_val.shape)\n",
    "        self.adaptive_constant_res_int_tf = tf.placeholder(tf.float32, shape=self.adaptive_constant_res_int_val.shape)\n",
    "        self.adaptive_constant_psi_int_tf = tf.placeholder(tf.float32, shape=self.adaptive_constant_psi_int_val.shape)\n",
    "        self.adaptive_constant_qz_int_tf = tf.placeholder(tf.float32, shape=self.adaptive_constant_qz_int_val.shape)\n",
    "        \n",
    "        # neural network definition\n",
    "        self.net_psi_1 = neural_net(self.t_res_1, self.z_res_1, layers = self.layers_psi_1, monotonic = False, normalize = self.normalize, LAA = self.LAA)\n",
    "        self.net_psi_2 = neural_net(self.t_res_2, self.z_res_2, layers = self.layers_psi_2, monotonic = False, normalize = self.normalize, LAA = self.LAA)\n",
    "     \n",
    "        # prediction at data points\n",
    "        \n",
    "        self.log_psi_data_1_pred = self.net_psi_1(self.t_data_1_tf, self.z_data_1_tf)\n",
    "        self.psi_data_1_pred = -tf.exp(self.log_psi_data_1_pred) + self.beta\n",
    "        self.theta_data_1_pred, self.K_data_1_pred = VGM(self.psi_data_1_pred, self.theta_r_1, self.theta_s_1, self.alpha_1, self.n_1, self.K_s_1, self.l_1)\n",
    "\n",
    "        self.log_psi_data_2_pred = self.net_psi_2(self.t_data_2_tf, self.z_data_2_tf)\n",
    "        self.psi_data_2_pred = -tf.exp(self.log_psi_data_2_pred) + self.beta\n",
    "        self.theta_data_2_pred, self.K_data_2_pred = VGM(self.psi_data_2_pred, self.theta_r_2, self.theta_s_2, self.alpha_2, self.n_2, self.K_s_2, self.l_2)\n",
    "                \n",
    "        # prediction at residual points\n",
    "        self.log_psi_res_1_pred = self.net_psi_1(self.t_res_1_tf, self.z_res_1_tf)\n",
    "        self.psi_res_1_pred = -tf.exp(self.log_psi_res_1_pred) + self.beta\n",
    "        self.theta_res_1_pred, self.K_res_1_pred = VGM(self.psi_res_1_pred, self.theta_r_1, self.theta_s_1, self.alpha_1, self.n_1, self.K_s_1, self.l_1)\n",
    "              \n",
    "        self.residual_res_1_pred, self.qz_res_1_pred = RRE_mixed_1D(self.psi_res_1_pred, self.theta_res_1_pred, self.K_res_1_pred,\n",
    "                                                                self.t_res_1_tf, self.z_res_1_tf)\n",
    "        \n",
    "        self.log_psi_res_2_pred = self.net_psi_2(self.t_res_2_tf, self.z_res_2_tf)\n",
    "        self.psi_res_2_pred = -tf.exp(self.log_psi_res_2_pred) + self.beta\n",
    "        self.theta_res_2_pred, self.K_res_2_pred = VGM(self.psi_res_2_pred, self.theta_r_2, self.theta_s_2, self.alpha_2, self.n_2, self.K_s_2, self.l_2)\n",
    "              \n",
    "        self.residual_res_2_pred, self.qz_res_2_pred = RRE_mixed_1D(self.psi_res_2_pred, self.theta_res_2_pred, self.K_res_2_pred,\n",
    "                                                                self.t_res_2_tf, self.z_res_2_tf)\n",
    "\n",
    "        # psi, flux, and residual at the layer interface\n",
    "        # layer 1\n",
    "        self.log_psi_int_1_pred = self.net_psi_1(self.t_int_tf, self.z_int_tf)\n",
    "        self.psi_int_1_pred = -tf.exp(self.log_psi_int_1_pred) + self.beta\n",
    "        self.theta_int_1_pred, self.K_int_1_pred = VGM(self.psi_int_1_pred, self.theta_r_1, self.theta_s_1, self.alpha_1, self.n_1, self.K_s_1, self.l_1)\n",
    "              \n",
    "        self.residual_int_1_pred, self.qz_int_1_pred = RRE_mixed_1D(self.psi_int_1_pred, self.theta_int_1_pred, self.K_int_1_pred,\n",
    "                                                                self.t_int_tf, self.z_int_tf)\n",
    "        \n",
    "        # layer 2\n",
    "        self.log_psi_int_2_pred = self.net_psi_2(self.t_int_tf,  self.z_int_tf)\n",
    "        self.psi_int_2_pred = -tf.exp(self.log_psi_int_2_pred) + self.beta\n",
    "        self.theta_int_2_pred, self.K_int_2_pred = VGM(self.psi_int_2_pred, self.theta_r_2, self.theta_s_2, self.alpha_2, self.n_2, self.K_s_2, self.l_2)\n",
    "              \n",
    "        self.residual_int_2_pred, self.qz_int_2_pred = RRE_mixed_1D(self.psi_int_2_pred, self.theta_int_2_pred, self.K_int_2_pred,\n",
    "                                                                self.t_int_tf, self.z_int_tf)\n",
    "                \n",
    "        # loss\n",
    "        # layer 1\n",
    "        self.loss_res_1 =  tf.reduce_mean(tf.square(self.residual_res_1_pred))\n",
    "        self.loss_data_1 = tf.reduce_mean(tf.square(self.theta_data_1_pred - self.theta_data_1_tf))\n",
    "        \n",
    "        self.loss_1 = self.loss_res_1 +  self.adaptive_constant_data_1_tf * self.loss_data_1 \n",
    "        \n",
    "        # layer 2\n",
    "        self.loss_res_2 =  tf.reduce_mean(tf.square(self.residual_res_2_pred))\n",
    "        self.loss_data_2 = tf.reduce_mean(tf.square(self.theta_data_2_pred - self.theta_data_2_tf))\n",
    "                \n",
    "        self.loss_2 = self.adaptive_constant_res_2_tf * self.loss_res_2 +  self.adaptive_constant_data_2_tf * self.loss_data_2\n",
    "        \n",
    "        self.loss_res_int = tf.reduce_mean(tf.square(self.residual_int_1_pred - self.residual_int_2_pred))\n",
    "        self.loss_psi_int = tf.reduce_mean(tf.square(self.log_psi_int_1_pred - self.log_psi_int_2_pred))\n",
    "        self.loss_qz_int = tf.reduce_mean(tf.square(self.qz_int_1_pred - self.qz_int_2_pred))\n",
    "        \n",
    "        self.loss_int = self.adaptive_constant_res_int_tf * self.loss_res_int  \\\n",
    "                         +  self.adaptive_constant_psi_int_tf * self.loss_psi_int  \\\n",
    "                         +  self.adaptive_constant_qz_int_tf * self.loss_qz_int\n",
    "        \n",
    "        \n",
    "        self.loss = self.loss_1 + self.loss_2 + self.loss_int\n",
    "        \n",
    "                    \n",
    "        # Define optimizer with learning rate schedule\n",
    "        self.global_step = tf.Variable(0, trainable = False)\n",
    "        starter_learning_rate = 1e-3\n",
    "        self.learning_rate = tf.train.exponential_decay(starter_learning_rate, self.global_step,\n",
    "                                                        1000, 0.90, staircase=False)\n",
    "        \n",
    "        # Passing global_step to minimize() will increment it at each step.\n",
    "        # self.train_op_1 = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss_1, global_step=self.global_step)\n",
    "        # self.train_op_2 = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss_2, global_step=self.global_step)\n",
    "        self.train_op = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss, global_step=self.global_step)\n",
    "        \n",
    "\n",
    "        # L-BFGS-B method\n",
    "        self.optimizer = tf.contrib.opt.ScipyOptimizerInterface(self.loss,\n",
    "                                                        method = 'L-BFGS-B',\n",
    "                                                        options = {'maxiter': 50000,\n",
    "                                                                   'maxfun': 50000,\n",
    "                                                                   'maxcor': 50,\n",
    "                                                                   'maxls': 50,\n",
    "                                                                   'ftol' : 1e-10,\n",
    "                                                                   'gtol' : 1e-8})\n",
    "        \n",
    "        \n",
    "        # Logger\n",
    "        \n",
    "        self.loss_res_1_log = []\n",
    "        self.loss_data_1_log = []\n",
    "        self.loss_res_2_log = []\n",
    "        self.loss_data_2_log = []\n",
    "        self.loss_psi_int_log = []\n",
    "        self.loss_qz_int_log = []\n",
    "        self.loss_res_int_log = []\n",
    "        \n",
    "        self.loss_res_1_log_BFGS = []\n",
    "        self.loss_data_1_log_BFGS = []\n",
    "        self.loss_res_2_log_BFGS = []\n",
    "        self.loss_data_2_log_BFGS = []\n",
    "        self.loss_psi_int_log_BFGS = []\n",
    "        self.loss_qz_int_log_BFGS = []\n",
    "        self.loss_res_int_log_BFGS = []\n",
    "        \n",
    "        self.saver = tf.train.Saver()\n",
    "        \n",
    "        # new residual points\n",
    "        self.new_residual_1 = []\n",
    "        self.new_residual_2 = []\n",
    "        \n",
    "        # Store the adaptive constant\n",
    "        \n",
    "        self.adaptive_constant_data_1_log = []\n",
    "        \n",
    "        self.adaptive_constant_res_2_log = []\n",
    "        self.adaptive_constant_data_2_log = []\n",
    "        \n",
    "        self.adaptive_constant_res_int_log = []\n",
    "        self.adaptive_constant_psi_int_log = []\n",
    "        self.adaptive_constant_qz_int_log = []\n",
    "        \n",
    "                \n",
    "        # Compute the adaptive constant\n",
    "        \n",
    "        self.adaptive_constant_data_1_list = []\n",
    "        \n",
    "        self.adaptive_constant_res_2_list = []\n",
    "        self.adaptive_constant_data_2_list = []\n",
    "        \n",
    "        self.adaptive_constant_res_int_list = []\n",
    "        self.adaptive_constant_psi_int_list = []\n",
    "        self.adaptive_constant_qz_int_list = []\n",
    "        \n",
    "        \n",
    "        self.sess = tf_session()\n",
    "\n",
    "    def train_Adam(self, iterations, batch = True, batch_size = 128):\n",
    "\n",
    "        N_res_1 = self.t_res_1.shape[0]\n",
    "        N_res_2 = self.t_res_2.shape[0]\n",
    "\n",
    "        start_time = timeit.default_timer()\n",
    "        running_time = 0\n",
    "        it = 0\n",
    "        for it in range(iterations):\n",
    "           \n",
    "            if batch:\n",
    "\n",
    "                idx_res_1 = np.random.choice(N_res_1, batch_size, replace = False)\n",
    "                idx_res_2 = np.random.choice(N_res_2, batch_size, replace = False)\n",
    "\n",
    "                (t_res_1, z_res_1) = (self.t_res_1[idx_res_1,:],\n",
    "                                  self.z_res_1[idx_res_1,:])\n",
    "                (t_res_2, z_res_2) = (self.t_res_2[idx_res_2,:],\n",
    "                                  self.z_res_2[idx_res_2,:])\n",
    "                \n",
    "            else:\n",
    "\n",
    "                (t_res_1, z_res_1) = (self.t_res_1,\n",
    "                                  self.z_res_1)\n",
    "                (t_res_2, z_res_2) = (self.t_res_2,\n",
    "                                  self.z_res_2)\n",
    "\n",
    "            tf_dict = {self.t_res_1_tf: t_res_1,\n",
    "                       self.z_res_1_tf: z_res_1,\n",
    "                       self.t_res_2_tf: t_res_2,\n",
    "                       self.z_res_2_tf: z_res_2,\n",
    "                       self.t_int_tf: self.t_int,\n",
    "                       self.z_int_tf: self.z_int,\n",
    "                       self.t_data_1_tf: self.t_data_1,\n",
    "                       self.z_data_1_tf: self.z_data_1,\n",
    "                       self.theta_data_1_tf: self.theta_data_1,\n",
    "                       self.t_data_2_tf: self.t_data_2,\n",
    "                       self.z_data_2_tf: self.z_data_2,\n",
    "                       self.theta_data_2_tf: self.theta_data_2,\n",
    "                       self.adaptive_constant_data_1_tf: self.adaptive_constant_data_1_val,\n",
    "                       self.adaptive_constant_res_2_tf: self.adaptive_constant_res_2_val,\n",
    "                       self.adaptive_constant_data_2_tf: self.adaptive_constant_data_2_val,\n",
    "                       self.adaptive_constant_res_int_tf: self.adaptive_constant_res_int_val,\n",
    "                       self.adaptive_constant_psi_int_tf: self.adaptive_constant_psi_int_val,\n",
    "                       self.adaptive_constant_qz_int_tf: self.adaptive_constant_qz_int_val}\n",
    "        \n",
    "        \n",
    "#             self.sess.run(self.train_op_1, tf_dict)\n",
    "#             self.sess.run(self.train_op_2, tf_dict)\n",
    "            self.sess.run(self.train_op, tf_dict)\n",
    "    \n",
    "            # Print\n",
    "            if it % 10 == 0:\n",
    "                elapsed = timeit.default_timer() - start_time\n",
    "                running_time += elapsed/3600.0\n",
    "                \n",
    "                \n",
    "        \n",
    "        \n",
    "                loss_res_1_value, loss_data_1_value = self.sess.run([self.loss_res_1, self.loss_data_1], tf_dict)\n",
    "                loss_res_2_value, loss_data_2_value = self.sess.run([self.loss_res_2, self.loss_data_2], tf_dict)\n",
    "\n",
    "                loss_psi_int_value, loss_qz_int_value, loss_res_int_value = self.sess.run([self.loss_psi_int, self.loss_qz_int, self.loss_res_int], tf_dict)\n",
    "                                                                                               \n",
    "                self.loss_res_1_log.append(loss_res_1_value)\n",
    "                self.loss_data_1_log.append(loss_data_1_value)\n",
    "                self.loss_res_2_log.append(loss_res_2_value)\n",
    "                self.loss_data_2_log.append(loss_data_2_value)\n",
    "                self.loss_psi_int_log.append(loss_psi_int_value)\n",
    "                self.loss_qz_int_log.append(loss_qz_int_value)\n",
    "                self.loss_res_int_log.append(loss_res_int_value)   \n",
    "                \n",
    "                print('It: %d, Loss_res_1: %.3e, Loss_data_1: %.3e,  \\\n",
    "                               Loss_res_2: %.3e, Loss_data_2: %.3e,  \\\n",
    "                               Loss_psi_int: %.3e, Loss_qz_int: %.3e, Loss_res_int: %.3e, Time: %.2f' %\n",
    "                      (it, loss_res_1_value, loss_data_1_value,\n",
    "                           loss_res_2_value, loss_data_2_value,\n",
    "                           loss_psi_int_value, loss_qz_int_value, loss_res_int_value, elapsed))\n",
    "                \n",
    "                start_time = timeit.default_timer()\n",
    "                                               \n",
    "    \n",
    "    def train_BFGS(self):\n",
    "        # full batch L-BFGS\n",
    "        tf_dict = {self.t_res_1_tf: self.t_res_1,\n",
    "                       self.z_res_1_tf: self.z_res_1,\n",
    "                       self.t_res_2_tf: self.t_res_2,\n",
    "                       self.z_res_2_tf: self.z_res_2,\n",
    "                       self.t_int_tf: self.t_int,\n",
    "                       self.z_int_tf: self.z_int,\n",
    "                       self.t_data_1_tf: self.t_data_1,\n",
    "                       self.z_data_1_tf: self.z_data_1,\n",
    "                       self.theta_data_1_tf: self.theta_data_1,\n",
    "                       self.t_data_2_tf: self.t_data_2,\n",
    "                       self.z_data_2_tf: self.z_data_2,\n",
    "                       self.theta_data_2_tf: self.theta_data_2,\n",
    "                       self.adaptive_constant_data_1_tf: self.adaptive_constant_data_1_val,\n",
    "                       self.adaptive_constant_res_2_tf: self.adaptive_constant_res_2_val,\n",
    "                       self.adaptive_constant_data_2_tf: self.adaptive_constant_data_2_val,\n",
    "                       self.adaptive_constant_res_int_tf: self.adaptive_constant_res_int_val,\n",
    "                       self.adaptive_constant_psi_int_tf: self.adaptive_constant_psi_int_val,\n",
    "                       self.adaptive_constant_qz_int_tf: self.adaptive_constant_qz_int_val}\n",
    "        \n",
    "#         L-BFGS-B\n",
    "        self.optimizer.minimize(self.sess,\n",
    "                                feed_dict = tf_dict,\n",
    "                                fetches = [self.loss_res_1, self.loss_data_1,\n",
    "                                           self.loss_res_2, self.loss_data_2,\n",
    "                                           self.loss_psi_int, self.loss_qz_int, self.loss_res_int],\n",
    "                                loss_callback = self.callback)\n",
    "        \n",
    "    def callback(self, loss_res_1_value, loss_data_1_value,\n",
    "                           loss_res_2_value, loss_data_2_value,\n",
    "                           loss_psi_int_value, loss_qz_int_value, loss_res_int_value):\n",
    "        \n",
    "        tf_dict = {self.t_res_1_tf: self.t_res_1,\n",
    "                       self.z_res_1_tf: self.z_res_1,\n",
    "                       self.t_res_2_tf: self.t_res_2,\n",
    "                       self.z_res_2_tf: self.z_res_2,\n",
    "                       self.t_int_tf: self.t_int,\n",
    "                       self.z_int_tf: self.z_int,\n",
    "                       self.t_data_1_tf: self.t_data_1,\n",
    "                       self.z_data_1_tf: self.z_data_1,\n",
    "                       self.theta_data_1_tf: self.theta_data_1,\n",
    "                       self.t_data_2_tf: self.t_data_2,\n",
    "                       self.z_data_2_tf: self.z_data_2,\n",
    "                       self.theta_data_2_tf: self.theta_data_2,\n",
    "                       self.adaptive_constant_data_1_tf: self.adaptive_constant_data_1_val,\n",
    "                       self.adaptive_constant_res_2_tf: self.adaptive_constant_res_2_val,\n",
    "                       self.adaptive_constant_data_2_tf: self.adaptive_constant_data_2_val,\n",
    "                       self.adaptive_constant_res_int_tf: self.adaptive_constant_res_int_val,\n",
    "                       self.adaptive_constant_psi_int_tf: self.adaptive_constant_psi_int_val,\n",
    "                       self.adaptive_constant_qz_int_tf: self.adaptive_constant_qz_int_val}\n",
    "        \n",
    "        self.loss_res_1_log_BFGS.append(loss_res_1_value)\n",
    "        self.loss_data_1_log_BFGS.append(loss_data_1_value)\n",
    "        self.loss_res_2_log_BFGS.append(loss_res_2_value)\n",
    "        self.loss_data_2_log_BFGS.append(loss_data_2_value)\n",
    "        self.loss_psi_int_log_BFGS.append(loss_psi_int_value)\n",
    "        self.loss_qz_int_log_BFGS.append(loss_qz_int_value)\n",
    "        self.loss_res_int_log_BFGS.append(loss_res_int_value)   \n",
    "                \n",
    "                \n",
    "        print('Loss_res_1: %.3e, Loss_data_1: %.3e,  \\n\\\n",
    "                               Loss_res_2: %.3e, Loss_data_2: %.3e,  \\n\\\n",
    "                               Loss_psi_int: %.3e, Loss_qz_int: %.3e, Loss_res_int: %.3e' %\n",
    "                      (loss_res_1_value, loss_data_1_value,\n",
    "                           loss_res_2_value, loss_data_2_value,\n",
    "                           loss_psi_int_value, loss_qz_int_value, loss_res_int_value))\n",
    "        \n",
    "        \n",
    "    def RAR(self, dom_coords_1, dom_coords_2, iterations, batchsize, m):\n",
    "        self.train_Adam(iterations, batchsize)\n",
    "        \n",
    "        i = 1\n",
    "        \n",
    "        while i < 10:\n",
    "            res_sampler_1 = Sampler(2, dom_coords_1, lambda x: 0)\n",
    "            res_sampler_2 = Sampler(2, dom_coords_2, lambda x: 0)\n",
    "            Z_res_1, _ = res_sampler_1.sample(np.int32(1e6))\n",
    "            Z_res_2, _ = res_sampler_2.sample(np.int32(1e6))\n",
    "            z_res_1, t_res_1 = Z_res_1[:, 0:1], Z_res_1[:, 1:2]\n",
    "            z_res_2, t_res_2 = Z_res_2[:, 0:1], Z_res_2[:, 1:2]\n",
    "        \n",
    "            tf_dict = {self.t_res_1_tf: t_res_1,\n",
    "                       self.z_res_1_tf: z_res_1,\n",
    "                       self.t_res_2_tf: t_res_2,\n",
    "                       self.z_res_2_tf: z_res_2}\n",
    "            residual_1 = self.sess.run(self.residual_res_1_pred, tf_dict)\n",
    "            residual_2 = self.sess.run(self.residual_res_2_pred, tf_dict)\n",
    "\n",
    "            err_abs_1 = np.abs(residual_1)\n",
    "            err_abs_2 = np.abs(residual_2)\n",
    "            err_1 = np.mean(err_abs_1)\n",
    "            err_2 = np.mean(err_abs_2)\n",
    "            print(\"Mean residual for layer 1: %.3e, Mean residual for layer 2: %.3e\" % (err_1, err_2))\n",
    "\n",
    "            # choose m largest residual\n",
    "            z_id_1 = err_abs_1.argsort(axis = 0)[-m * i:][::-1][:,0]\n",
    "            z_id_2 = err_abs_2.argsort(axis = 0)[-m * i:][::-1][:,0]\n",
    "\n",
    "            print(z_id_1, z_id_2)\n",
    "            print(\"Adding new point for layer 1:\", Z_res_1[z_id_1, :], \"\\n\")\n",
    "            print(\"High residuals for layer 1 are:\", err_abs_1[z_id_1], \"\\n\")\n",
    "            print(\"Adding new point for layer 2:\", Z_res_2[z_id_2, :], \"\\n\")\n",
    "            print(\"High residuals for layer 2 are:\", err_abs_2[z_id_2], \"\\n\")\n",
    "        \n",
    "            self.new_residual_1.append(Z_res_1[z_id_1, :])\n",
    "            self.new_residual_2.append(Z_res_2[z_id_2, :])\n",
    "            \n",
    "            self.z_res_1 = np.vstack((self.z_res_1, Z_res_1[z_id_1, 0:1]))\n",
    "            self.t_res_1 = np.vstack((self.t_res_1, Z_res_1[z_id_1, 1:2]))\n",
    "            \n",
    "            self.z_res_2 = np.vstack((self.z_res_2, Z_res_2[z_id_2, 0:1]))\n",
    "            self.t_res_2 = np.vstack((self.t_res_2, Z_res_2[z_id_2, 1:2]))\n",
    "            \n",
    "            self.train_Adam(iterations, batchsize)\n",
    "            \n",
    "            i += 1\n",
    "\n",
    "            print(i)\n",
    "            \n",
    "    def predict_1(self, t_star, z_star):\n",
    "\n",
    "        tf_dict = {self.t_res_1_tf: t_star, self.z_res_1_tf: z_star}\n",
    "\n",
    "        psi_star = self.sess.run(self.psi_res_1_pred, tf_dict)\n",
    "        theta_star = self.sess.run(self.theta_res_1_pred, tf_dict)\n",
    "        K_star = self.sess.run(self.K_res_1_pred, tf_dict)\n",
    "        qz_star = self.sess.run(self.qz_res_1_pred, tf_dict)\n",
    "        \n",
    "        residual = self.sess.run(self.residual_res_1_pred, tf_dict)\n",
    "\n",
    "        return psi_star, theta_star, K_star, qz_star, residual\n",
    "    \n",
    "    def predict_2(self, t_star, z_star):\n",
    "\n",
    "        tf_dict = {self.t_res_2_tf: t_star, self.z_res_2_tf: z_star}\n",
    "\n",
    "        psi_star = self.sess.run(self.psi_res_2_pred, tf_dict)\n",
    "        theta_star = self.sess.run(self.theta_res_2_pred, tf_dict)\n",
    "        K_star = self.sess.run(self.K_res_2_pred, tf_dict)\n",
    "        qz_star = self.sess.run(self.qz_res_2_pred, tf_dict)\n",
    "        \n",
    "        residual = self.sess.run(self.residual_res_2_pred, tf_dict)\n",
    "\n",
    "        return psi_star, theta_star, K_star, qz_star, residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dom_coords_1 = np.array([[0.0, 0.0],   # t = 0 hour to t = 20 hours\n",
    "#                        [20.0, -10.0]])\n",
    "# dom_coords_2 = np.array([[0.0, 10.0],   # t = 0 hour to t = 20 hours\n",
    "#                        [20.0, 0.0]])\n",
    "# int_coords = np.array([[0.0, 0.0],\n",
    "#                        [20.0, 0.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# produce residual points \n",
    "# res_sampler_1 = Sampler(2, dom_coords_1, lambda x: 0)\n",
    "# res_sampler_2 = Sampler(2, dom_coords_2, lambda x: 0)\n",
    "# int_sampler = Sampler(2, int_coords, lambda x: 0)\n",
    "# # np.random.seed(0)\n",
    "# Z_res_1, _ = res_sampler_1.sample(np.int32(100000))\n",
    "# Z_res_2, _ = res_sampler_2.sample(np.int32(100000))\n",
    "# Z_int, _ = int_sampler.sample(np.int32(10000))\n",
    "# np.save(\"data/inverse_residual_points_layer1\", Z_res_1)\n",
    "# np.save(\"data/inverse_residual_points_layer2\", Z_res_2)\n",
    "# np.save(\"data/inverse_residual_points_interface\", Z_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create noisy data\n",
    "# theta_true = np.load(f\"../analytical_solutions/hydrus_example_theta.npy\")\n",
    "# np.random.seed(0)\n",
    "\n",
    "# noise = 0.005\n",
    "# noise_theta = noise*np.random.randn(theta_true.shape[0], theta_true.shape[1])\n",
    "# theta_noise = theta_true + noise_theta\n",
    "\n",
    "# np.save(\"../analytical_solutions/hydrus_example_noisy_theta.npy\", theta_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_loop(folder, ID, random_seed, data_depth, number_int = 1000, LAA = True, ALR = False, weight_values = [10, 10, 10, 1, 1, 1],\n",
    "              number_layers_1 = 5, number_units_1 = 50, number_layers_2 = 5, number_units_2 = 50, \n",
    "              number_res_1 = 10000, number_res_2 = 10000,\n",
    "              RAR = False, iterations = 100000, batch_size = 128, new_residual_points = 10):\n",
    "    \"\"\"\n",
    "    ALR: adaptive learning rate\n",
    "    LAA: layer-wise locally adaptive activation functions\n",
    "    RAR: residual adaptive refinement\n",
    "    weight_values: [data_1, data_2, res_2, res_int, psi_int, qz_int]\n",
    "    data_depth: mm from the top of the soil (e.g., 10 means 1 cm from the top)\n",
    "    \"\"\"\n",
    "    \n",
    "    if not os.path.isdir(f'../results/{folder}'):\n",
    "        os.mkdir(f'../results/{folder}')\n",
    "\n",
    "    if not os.path.isdir(f'../results/{folder}/{ID}'):\n",
    "        os.mkdir(f'../results/{folder}/{ID}')\n",
    "\n",
    "    # domain boundaries; the first column is time, the second column is depth (positive upward)\n",
    "\n",
    "    dom_coords_1 = np.array([[0.0, 0.0],   \n",
    "                           [20.0, -10.0]])\n",
    "    dom_coords_2 = np.array([[0.0, 10.0],  \n",
    "                           [20.0, 0.0]])\n",
    "    int_coords = np.array([[0.0, 0.0],\n",
    "                           [20.0, 0.0]])\n",
    "\n",
    "    # import residual points \n",
    "    Z_res_1 = np.load(f\"data/inverse_residual_points_layer1.npy\")\n",
    "    Z_res_2 = np.load(f\"data/inverse_residual_points_layer2.npy\")\n",
    "    Z_int = np.load(f\"data/inverse_residual_points_interface.npy\")\n",
    "\n",
    "    # hydraulic paramters\n",
    "    hydraulic_parameters_1 = VGM_parameters[\"layer1\"]\n",
    "    hydraulic_parameters_2 = VGM_parameters[\"layer2\"]\n",
    "\n",
    "\n",
    "    # hydralic function model\n",
    "    hydraulic_model = \"VGM\"\n",
    "\n",
    "    # normalization\n",
    "    normalize = False\n",
    "\n",
    "    # NN architecture\n",
    "    layers_psi_1 = [2] + number_layers_1*[number_units_1] + [1]\n",
    "    layers_psi_2 = [2] + number_layers_2*[number_units_2] + [1]\n",
    "\n",
    "    # forward solution\n",
    "    # collocation points\n",
    "    if RAR:\n",
    "        number_res_1_2 =  number_res_1 - (RAR_iterations - 1) * 10\n",
    "        number_res_2_2 =  number_res_2 - (RAR_iterations - 1) * 10\n",
    "    else:\n",
    "        number_res_1_2 = number_res_1\n",
    "        number_res_2_2 = number_res_2\n",
    "        \n",
    "    t_res_1 = Z_res_1[:number_res_1_2, 0:1]\n",
    "    z_res_1 = Z_res_1[:number_res_1_2, 1:2]\n",
    "\n",
    "    t_res_2 = Z_res_2[:number_res_2_2, 0:1]\n",
    "    z_res_2 = Z_res_2[:number_res_2_2, 1:2]\n",
    "\n",
    "    t_res = np.concatenate((t_res_2, t_res_1), axis = 1)\n",
    "    z_res = np.concatenate((z_res_2, z_res_1), axis = 1) \n",
    "    \n",
    "    # the first column is the top node \n",
    "    theta_true = np.load(f\"../analytical_solutions/hydrus_example_theta.npy\")\n",
    "    theta_noise = np.load(f\"../analytical_solutions/hydrus_example_noisy_theta.npy\")\n",
    "    data_depth_1 = [depth for depth in data_depth if depth > 100]\n",
    "    data_depth_2 = [depth for depth in data_depth if depth < 100]\n",
    "\n",
    "    theta_data_1 = theta_noise[:, [i + 1 for i in data_depth_1]].flatten(order = 'F')[:, None]\n",
    "    theta_data_2 = theta_noise[:, data_depth_2].flatten(order = 'F')[:, None]\n",
    "    \n",
    "    T = 201\n",
    "    N = 101\n",
    "    t_1 = np.linspace(dom_coords_1[0, 0], dom_coords_1[1, 0], T)[:, None]\n",
    "    z_1 = np.linspace(dom_coords_1[0, 1], dom_coords_1[1, 1], N)[:, None]\n",
    "    z_1_mesh, t_1_mesh = np.meshgrid(z_1, t_1)\n",
    "\n",
    "    t_2 = np.linspace(dom_coords_2[0, 0], dom_coords_2[1, 0], T)[:, None]\n",
    "    z_2 = np.linspace(dom_coords_2[0, 1], dom_coords_2[1, 1], N)[:, None]\n",
    "    z_2_mesh, t_2_mesh = np.meshgrid(z_2, t_2)\n",
    "\n",
    "    t_data_1 = t_1_mesh[:,[i - 100 for i in data_depth_1]].flatten(order = 'F')[:, None]\n",
    "    t_data_2 = t_2_mesh[:,data_depth_2].flatten(order = 'F')[:, None]\n",
    "    \n",
    "    z_data_1 = z_1_mesh[:, [i - 100 for i in data_depth_1]].flatten(order = 'F')[:, None]\n",
    "    z_data_2 = z_2_mesh[:, data_depth_2].flatten(order = 'F')[:, None]\n",
    "    \n",
    "    t_int = Z_int[:number_int, 0:1]\n",
    "    z_int = Z_int[:number_int, 1:2]\n",
    "\n",
    "\n",
    "    # initialization\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(random_seed)\n",
    "    random.seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "\n",
    "    # training    \n",
    "    model = PINNs_RRE1D(\n",
    "                t_res_1, z_res_1,\n",
    "                t_res_2, z_res_2,\n",
    "                t_int, z_int,\n",
    "                t_data_1, z_data_1, theta_data_1,\n",
    "                t_data_2, z_data_2, theta_data_2, \n",
    "                layers_psi_1, layers_psi_2, hydraulic_model, hydraulic_parameters_1, hydraulic_parameters_2,\n",
    "                weight_values, normalize, ALR, LAA)\n",
    "\n",
    "    # Adam training\n",
    "    start_time = timeit.default_timer() \n",
    "\n",
    "    if RAR:\n",
    "        iterations = iterations//10\n",
    "        model.RAR(dom_coords, iterations, batch_size, new_residual_points)\n",
    "    else:\n",
    "        model.train_Adam(iterations, batch_size)\n",
    "    time_Adam = timeit.default_timer() - start_time\n",
    "\n",
    "    # BFGS training\n",
    "    start_time = timeit.default_timer() \n",
    "    model.train_BFGS()\n",
    "    time_BFGS = timeit.default_timer() - start_time\n",
    "\n",
    "    # evaluation\n",
    "    T = 201\n",
    "    N = 101\n",
    "    t_1 = np.linspace(dom_coords_1[0, 0], dom_coords_1[1, 0], T)[:, None]\n",
    "    z_1 = np.linspace(dom_coords_1[0, 1], dom_coords_1[1, 1], N)[:, None]\n",
    "    z_1_mesh, t_1_mesh = np.meshgrid(z_1, t_1)\n",
    "    X_star_1 = np.hstack((t_1_mesh.flatten()[:, None], z_1_mesh.flatten()[:, None]))\n",
    "\n",
    "    t_2 = np.linspace(dom_coords_2[0, 0], dom_coords_2[1, 0], T)[:, None]\n",
    "    z_2 = np.linspace(dom_coords_2[0, 1], dom_coords_2[1, 1], N)[:, None]\n",
    "    z_2_mesh, t_2_mesh = np.meshgrid(z_2, t_2)\n",
    "    X_star_2 = np.hstack((t_2_mesh.flatten()[:, None], z_2_mesh.flatten()[:, None]))\n",
    "\n",
    "    t_test_1 = X_star_1[:, 0:1]\n",
    "    z_test_1 = X_star_1[:, 1:2]\n",
    "\n",
    "    t_test_2 = X_star_2[:, 0:1]\n",
    "    z_test_2 = X_star_2[:, 1:2]\n",
    "\n",
    "    # prediction\n",
    "    psi_1_pred, theta_1_pred, K_1_pred, flux_1_pred, residual_1 = model.predict_1(t_test_1, z_test_1)\n",
    "    psi_2_pred, theta_2_pred, K_2_pred, flux_2_pred, residual_2 = model.predict_2(t_test_2, z_test_2)\n",
    "\n",
    "    theta_1_pred = theta_1_pred.reshape((T, N))\n",
    "    K_1_pred = K_1_pred.reshape((T, N))\n",
    "    psi_1_pred = psi_1_pred.reshape((T, N))\n",
    "    flux_1_pred = flux_1_pred.reshape((T, N))\n",
    "    residual_1 = residual_1.reshape((T, N))\n",
    "\n",
    "    theta_2_pred = theta_2_pred.reshape((T, N))\n",
    "    K_2_pred = K_2_pred.reshape((T, N))\n",
    "    psi_2_pred = psi_2_pred.reshape((T, N))\n",
    "    flux_2_pred = flux_2_pred.reshape((T, N))\n",
    "    residual_2 = residual_2.reshape((T, N))\n",
    "\n",
    "    psi_pred = np.concatenate((psi_2_pred, psi_1_pred), axis = 1) \n",
    "    theta_pred = np.concatenate((theta_2_pred, theta_1_pred), axis = 1)\n",
    "    K_pred = np.concatenate((K_2_pred, K_1_pred), axis = 1)\n",
    "    residual = np.concatenate((residual_2, residual_1), axis = 1)\n",
    "    flux_pred = np.concatenate((flux_2_pred, flux_1_pred), axis = 1) \n",
    "\n",
    "    t_mesh = np.concatenate((t_2_mesh, t_1_mesh), axis = 1)\n",
    "    z_mesh = np.concatenate((z_2_mesh, z_1_mesh), axis = 1) \n",
    "        \n",
    "\n",
    "    # import true solution: first index is depth, second index is time\n",
    "    psi_true = np.load(f\"../analytical_solutions/hydrus_example_psi.npy\")\n",
    "    theta_true = np.load(f\"../analytical_solutions/hydrus_example_theta.npy\")\n",
    "\n",
    "    # the error at the interface is computed \"twice\" (theta in layer 1 and layer 2, which are not necesarily the same)\n",
    "    theta_L2_error = np.sqrt(np.mean((theta_true - theta_pred) ** 2))\n",
    "    theta_abs_error = np.mean(np.abs(theta_true- theta_pred))\n",
    "    theta_square = np.sqrt(np.mean((theta_true) ** 2))\n",
    "    theta_max_error = np.max(np.abs(theta_true- theta_pred))\n",
    "\n",
    "    theta_L2_relative_error = theta_L2_error/theta_square\n",
    "    theta_abs_relative_error = theta_abs_error/np.mean(np.abs(theta_true.T))\n",
    "\n",
    "    print(\"theta_L2_relative_error is\", theta_L2_relative_error)\n",
    "    print(\"theta_abs_relative_error is\", theta_abs_relative_error)\n",
    "    print(\"theta_max_error is\", theta_max_error)\n",
    "\n",
    "    psi_L2_error = np.sqrt(np.mean((psi_true - psi_pred) ** 2))\n",
    "    psi_abs_error = np.mean(np.abs(psi_true- psi_pred))\n",
    "    psi_square = np.sqrt(np.mean((psi_true) ** 2))\n",
    "    psi_max_error = np.max(np.abs(psi_true- psi_pred))\n",
    "\n",
    "    psi_L2_relative_error = psi_L2_error/psi_square\n",
    "    psi_abs_relative_error = psi_abs_error/np.mean(np.abs(psi_true.T))\n",
    "\n",
    "\n",
    "    print(\"psi_L2_relative_error is\", psi_L2_relative_error)\n",
    "    print(\"psi_abs_relative_error is\", psi_abs_relative_error)\n",
    "    print(\"psi_max_error is\", psi_max_error)\n",
    "    \n",
    "    # theta distribution\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "    c1 = ax[0].pcolor(t_mesh, z_mesh - 10.0, theta_pred, cmap='gnuplot2_r')\n",
    "    fig.colorbar(c1, ax = ax[0])\n",
    "    ax[0].set_xlabel(r'$t$ [hours]')\n",
    "    ax[0].set_ylabel(r'$z$ [cm]')\n",
    "    ax[0].set_title('Predicted $\\\\theta$')\n",
    "\n",
    "    c2 = ax[1].pcolor(t_mesh, z_mesh - 10.0, theta_true, cmap='gnuplot2_r')\n",
    "    fig.colorbar(c2, ax = ax[1])\n",
    "    ax[1].set_xlabel(r'$t$ [hours]')\n",
    "    ax[1].set_title('True $\\\\theta$')\n",
    "\n",
    "    c3 = ax[2].pcolor(t_mesh, z_mesh - 10.0, np.abs(theta_pred - theta_true), cmap='gnuplot2_r')\n",
    "    fig.colorbar(c3, ax = ax[2])\n",
    "    ax[2].set_xlabel(r'$t$ [hours]')\n",
    "    ax[2].set_title('Absolute error in $\\\\theta$')\n",
    "\n",
    "    fig.savefig(f'../results/{folder}/{ID}/theta_color.png')\n",
    "    fig.clf()\n",
    "\n",
    "    # psi distribution\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "    c1 = ax[0].pcolor(t_mesh, z_mesh - 10.0, psi_pred, cmap='gnuplot2_r')\n",
    "    fig.colorbar(c1, ax = ax[0])\n",
    "    ax[0].set_xlabel(r'$t$ [hours]')\n",
    "    ax[0].set_ylabel(r'$z$ [cm]')\n",
    "    ax[0].set_title('Predicted $\\\\psi$ [cm]')\n",
    "\n",
    "    c2 = ax[1].pcolor(t_mesh, z_mesh - 10.0, psi_true, cmap='gnuplot2_r')\n",
    "    fig.colorbar(c2, ax = ax[1])\n",
    "    ax[1].set_xlabel(r'$t$ [hours]')\n",
    "    ax[1].set_title('True $\\\\psi$ [cm]')\n",
    "\n",
    "    c3 = ax[2].pcolor(t_mesh, z_mesh - 10.0, np.abs(psi_pred - psi_true), cmap='gnuplot2_r')\n",
    "    fig.colorbar(c3, ax = ax[2])\n",
    "    ax[2].set_xlabel(r'$t$ [hours]')\n",
    "    ax[2].set_title('Absolute error in $\\\\psi$ [cm]')\n",
    "\n",
    "    fig.savefig(f'../results/{folder}/{ID}/psi_color.png')\n",
    "    fig.clf()\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(6.5, 6.5))\n",
    "\n",
    "    c1 = ax.pcolor(t_mesh, z_mesh - 10.0, flux_pred, cmap='gnuplot2_r')\n",
    "    fig.colorbar(c1, ax = ax)\n",
    "    ax.set_xlabel(r'$t$ [hours]')\n",
    "    ax.set_ylabel(r'$z$ [cm]')\n",
    "    ax.set_title('Predicted $q$ [cm/hours]')\n",
    "\n",
    "    fig.savefig(f'../results/{folder}/{ID}/flux.png')\n",
    "    fig.clf()\n",
    "\n",
    "    fig = plt.subplots(1, 1, figsize=(6.5, 6.5))\n",
    "\n",
    "    plt.scatter(theta_pred[0, ::5], z_mesh[0, ::5] - 10.0, s = 15, label = 'Predicted')\n",
    "    plt.scatter(theta_pred[1, ::5], z_mesh[0, ::5] - 10.0, s = 15)\n",
    "    plt.scatter(theta_pred[5, ::5], z_mesh[0, ::5] - 10.0, s = 15)\n",
    "    plt.scatter(theta_pred[10, ::5], z_mesh[0, ::5] - 10.0, s = 15)\n",
    "    plt.scatter(theta_pred[30, ::5], z_mesh[0, ::5] - 10.0, s = 15)\n",
    "    plt.scatter(theta_pred[50, ::5], z_mesh[0, ::5] - 10.0, s = 15)\n",
    "    plt.scatter(theta_pred[100, ::5], z_mesh[0, ::5] - 10.0, s = 15)\n",
    "    plt.scatter(theta_pred[150, ::5], z_mesh[0, ::5] - 10.0, s = 15)\n",
    "    plt.scatter(theta_pred[200, ::5], z_mesh[0, ::5] - 10.0, s = 15)\n",
    "\n",
    "    plt.plot(theta_true[0, :], z_mesh[0, :] - 10.0 ,label = 't = 0 hour')\n",
    "    plt.plot(theta_true[1, :], z_mesh[0, :] - 10.0,label = 't = 0.1 hour')\n",
    "    plt.plot(theta_true[5, :], z_mesh[0, :] - 10.0,label = 't = 0.5 hour')\n",
    "    plt.plot(theta_true[10, :], z_mesh[0, :]- 10.0 ,label = 't = 1 hour')\n",
    "    plt.plot(theta_true[30, :], z_mesh[0, :]- 10.0,label = 't = 3 hour')\n",
    "    plt.plot(theta_true[50, :], z_mesh[0, :]- 10.0,label = 't = 5 hour')\n",
    "    plt.plot(theta_true[100, :], z_mesh[0, :]- 10.0, label = 't = 10 hour')\n",
    "    plt.plot(theta_true[150, :], z_mesh[0, :]- 10.0, label = 't = 15 hour')\n",
    "    plt.plot(theta_true[200, :], z_mesh[0, :]- 10.0, label = 't = 20 hour')\n",
    "\n",
    "    plt.xlim(0.05,0.50)\n",
    "    plt.xlabel('$\\\\theta$')\n",
    "    plt.ylabel('$z$ cm')\n",
    "    plt.legend(loc = (0.75,0.55), fontsize = 8)\n",
    "\n",
    "    plt.savefig(f'../results/{folder}/{ID}/theta_fixed_time.png')\n",
    "    plt.clf()\n",
    "\n",
    "    t_res = np.concatenate((t_res_2, t_res_1), axis = 1)\n",
    "    z_res = np.concatenate((z_res_2, z_res_1), axis = 1) \n",
    "\n",
    "    fig = plt.figure(1, figsize=(6.5, 6.5))\n",
    "\n",
    "    plt.plot(t_res, z_res - 10.0, 'rx', markersize = 3, label = \"residual points\")\n",
    "\n",
    "    if RAR:\n",
    "        new_residual = model.new_residual[0]\n",
    "        for i in range(1, len(model.new_residual)):\n",
    "            new_residual = np.vstack((new_residual, model.new_residual[i]))\n",
    "        plt.plot(new_residual[:, 0], new_residual[:, 1] - 10.0, 'bx', markersize = 5, label = \"added residual points\")\n",
    "\n",
    "\n",
    "\n",
    "    plt.xlabel(r'$t$ [hours]')\n",
    "    plt.ylabel(r'$z$ [cm]')\n",
    "    plt.title('residual points')\n",
    "    plt.xlim(-0.1, 10.1)\n",
    "    plt.ylim(-10.1, 0.1)\n",
    "    plt.legend(loc = (0.55,0.15))\n",
    "    plt.savefig(f'../results/{folder}/{ID}/residual_points.png', transparent = True)\n",
    "    plt.clf()\n",
    "\n",
    "    fig = plt.figure(1, figsize=(6.5, 6.5))\n",
    "\n",
    "    plt.pcolor(t_mesh, z_mesh - 10.0, np.log10(np.abs(residual)), cmap='gnuplot2_r')\n",
    "\n",
    "    plt.colorbar()\n",
    "    plt.xlabel(r'$t$ [hours]')\n",
    "    plt.ylabel(r'$z$ [cm]')\n",
    "    plt.title('$log_{10}r(x)$')\n",
    "    plt.savefig(f'../results/{folder}/{ID}/residual.png', transparent = True)\n",
    "    plt.clf()\n",
    "\n",
    "    # Loss\n",
    "\n",
    "    loss_res_1 = model.loss_res_1_log\n",
    "    loss_data_1 = model.loss_data_1_log\n",
    "    loss_res_2 = model.loss_res_2_log\n",
    "    loss_data_2 = model.loss_data_2_log\n",
    "    loss_psi_int = model.loss_psi_int_log\n",
    "    loss_qz_int = model.loss_qz_int_log\n",
    "    loss_res_int = model.loss_res_int_log\n",
    "\n",
    "    Adam_iterations = np.arange(0, len(loss_res_1)) * 10\n",
    "\n",
    "    loss_res_1_BFGS = model.loss_res_1_log_BFGS\n",
    "    loss_data_1_BFGS = model.loss_data_1_log_BFGS\n",
    "    loss_res_2_BFGS = model.loss_res_2_log_BFGS\n",
    "    loss_data_2_BFGS = model.loss_data_2_log_BFGS\n",
    "    loss_psi_int_BFGS = model.loss_psi_int_log_BFGS\n",
    "    loss_qz_int_BFGS = model.loss_qz_int_log_BFGS\n",
    "    loss_res_int_BFGS = model.loss_res_int_log_BFGS\n",
    "\n",
    "    BFGS_iterations = np.arange(0, len(loss_res_1_BFGS))\n",
    "\n",
    "    total_iterations = np.append(Adam_iterations, BFGS_iterations + Adam_iterations[-1])\n",
    "\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "    # Adam\n",
    "    ax[0].plot(Adam_iterations, loss_data_1, label='$\\mathcal{L}_{data1}$')\n",
    "    ax[0].plot(Adam_iterations, loss_data_2, label='$\\mathcal{L}_{data2}$')\n",
    "    ax[0].plot(Adam_iterations, loss_res_1, label='$\\mathcal{L}_{res1}$')\n",
    "    ax[0].plot(Adam_iterations, loss_res_2, label='$\\mathcal{L}_{res2}$')\n",
    "    ax[0].plot(Adam_iterations, loss_psi_int, label='$\\mathcal{L}_{\\psi-int}$')\n",
    "    ax[0].plot(Adam_iterations, loss_qz_int, label='$\\mathcal{L}_{qz-int}$')\n",
    "    ax[0].plot(Adam_iterations, loss_res_int, label='$\\mathcal{L}_{res-int}$')\n",
    "    ax[0].set_yscale('log')\n",
    "    ax[0].set_xlabel('iterations')\n",
    "    ax[0].set_ylabel('Loss')\n",
    "    ax[0].set_title('Adam')\n",
    "    ax[0].legend()\n",
    "\n",
    "    # BFGS\n",
    "    ax[1].plot(BFGS_iterations, loss_data_1_BFGS, label='$\\mathcal{L}_{data1}$')\n",
    "    ax[1].plot(BFGS_iterations, loss_data_2_BFGS, label='$\\mathcal{L}_{data2}$')\n",
    "    ax[1].plot(BFGS_iterations, loss_res_1_BFGS, label='$\\mathcal{L}_{res1}$')\n",
    "    ax[1].plot(BFGS_iterations, loss_res_2_BFGS, label='$\\mathcal{L}_{res2}$')\n",
    "    ax[1].plot(BFGS_iterations, loss_psi_int_BFGS, label='$\\mathcal{L}_{\\psi-int}$')\n",
    "    ax[1].plot(BFGS_iterations, loss_qz_int_BFGS, label='$\\mathcal{L}_{qz-int}$')\n",
    "    ax[1].plot(BFGS_iterations, loss_res_int_BFGS, label='$\\mathcal{L}_{res-int}$')\n",
    "\n",
    "    ax[1].set_yscale('log')\n",
    "    ax[1].set_xlabel('iterations')\n",
    "    ax[1].set_ylabel('Loss')\n",
    "    ax[1].set_title('L-BFGS-B')\n",
    "    ax[1].legend()\n",
    "\n",
    "\n",
    "    # total training\n",
    "    ax[2].plot(total_iterations, loss_data_1 + loss_data_1_BFGS, label='$\\mathcal{L}_{data1}$')\n",
    "    ax[2].plot(total_iterations, loss_data_2 + loss_data_2_BFGS, label='$\\mathcal{L}_{data2}$')\n",
    "    ax[2].plot(total_iterations, loss_res_1 + loss_res_1_BFGS, label='$\\mathcal{L}_{res1}$')\n",
    "    ax[2].plot(total_iterations, loss_res_2 + loss_res_2_BFGS, label='$\\mathcal{L}_{res2}$')\n",
    "    ax[2].plot(total_iterations, loss_psi_int + loss_psi_int_BFGS, label='$\\mathcal{L}_{\\psi-int}$')\n",
    "    ax[2].plot(total_iterations, loss_qz_int + loss_qz_int_BFGS, label='$\\mathcal{L}_{qz-int}$')\n",
    "    ax[2].plot(total_iterations, loss_res_int + loss_res_int_BFGS, label='$\\mathcal{L}_{res-int}$')\n",
    "\n",
    "\n",
    "    ax[2].set_yscale('log')\n",
    "    ax[2].set_xlabel('iterations')\n",
    "    ax[2].set_ylabel('Loss')\n",
    "    ax[2].set_title('total')\n",
    "    ax[2].legend()\n",
    "\n",
    "    plt.savefig(f'../results/{folder}/{ID}/training.png')\n",
    "    plt.clf()\n",
    "\n",
    "    np.save(f\"../results/{folder}/{ID}/theta_pred\", theta_pred)\n",
    "    np.save(f\"../results/{folder}/{ID}/psi_pred\", psi_pred)\n",
    "    np.save(f\"../results/{folder}/{ID}/K_pred\", K_pred)\n",
    "    np.save(f\"../results/{folder}/{ID}/flux_pred\", flux_pred)\n",
    "    np.save(f\"../results/{folder}/{ID}/residual\", residual)\n",
    "\n",
    "    np.save(f\"../results/{folder}/{ID}/loss_data_1\", loss_data_1)\n",
    "    np.save(f\"../results/{folder}/{ID}/loss_data_1_BFGS\", loss_data_1_BFGS)\n",
    "    np.save(f\"../results/{folder}/{ID}/loss_data_2\", loss_data_1)\n",
    "    np.save(f\"../results/{folder}/{ID}/loss_data_2_BFGS\", loss_data_2_BFGS)\n",
    "\n",
    "    np.save(f\"../results/{folder}/{ID}/loss_res_1\", loss_res_1)\n",
    "    np.save(f\"../results/{folder}/{ID}/loss_res_1_BFGS\", loss_res_1_BFGS)\n",
    "    np.save(f\"../results/{folder}/{ID}/loss_res_2\", loss_res_2)\n",
    "    np.save(f\"../results/{folder}/{ID}/loss_res_2_BFGS\", loss_res_2_BFGS)\n",
    "\n",
    "    np.save(f\"../results/{folder}/{ID}/loss_psi_int\", loss_psi_int)\n",
    "    np.save(f\"../results/{folder}/{ID}/loss_psi_int_BFGS\", loss_psi_int_BFGS)\n",
    "    np.save(f\"../results/{folder}/{ID}/loss_qz_int\", loss_qz_int)\n",
    "    np.save(f\"../results/{folder}/{ID}/loss_qz_int_BFGS\", loss_qz_int_BFGS)\n",
    "    np.save(f\"../results/{folder}/{ID}/loss_res_int\", loss_res_int)\n",
    "    np.save(f\"../results/{folder}/{ID}/loss_res_int_BFGS\", loss_res_int_BFGS)\n",
    "\n",
    "\n",
    "    np.save(f\"../results/{folder}/{ID}/t_res\", t_res)\n",
    "    np.save(f\"../results/{folder}/{ID}/z_res\", z_res)\n",
    "    if RAR:\n",
    "        np.save(f\"../results/{folder}/{ID}/new_residual\", new_residual)\n",
    "\n",
    "    np.save(f\"../results/{folder}/{ID}/z_mesh\", z_mesh)\n",
    "    np.save(f\"../results/{folder}/{ID}/t_mesh\", t_mesh)\n",
    "\n",
    "\n",
    "    sheet1.write(ID, 0, ID)\n",
    "    sheet1.write(ID, 1, random_seed)\n",
    "    sheet1.write(ID, 2, number_layers_1)\n",
    "    sheet1.write(ID, 3, number_units_1)\n",
    "    sheet1.write(ID, 4, number_layers_2)\n",
    "    sheet1.write(ID, 5, number_units_2)\n",
    "    sheet1.write(ID, 6, LAA)\n",
    "    sheet1.write(ID, 7, number_res_1)\n",
    "    sheet1.write(ID, 8, number_res_2)\n",
    "    sheet1.write(ID, 9, str(data_depth))\n",
    "    sheet1.write(ID, 10, number_int)\n",
    "\n",
    "    sheet1.write(ID, 11, weight_values[0])\n",
    "    sheet1.write(ID, 12, weight_values[1])\n",
    "    sheet1.write(ID, 13, weight_values[2])\n",
    "    sheet1.write(ID, 14, weight_values[3])\n",
    "    sheet1.write(ID, 15, weight_values[4])\n",
    "    sheet1.write(ID, 16, weight_values[5])\n",
    "\n",
    "    sheet1.write(ID, 17, ALR)\n",
    "    sheet1.write(ID, 18, iterations)\n",
    "    sheet1.write(ID, 19, batch_size)\n",
    "    sheet1.write(ID, 20, RAR)\n",
    "    sheet1.write(ID, 21, new_residual_points)\n",
    "    sheet1.write(ID, 22, time_Adam)\n",
    "    sheet1.write(ID, 23, time_BFGS)\n",
    "\n",
    "    sheet1.write(ID, 24, theta_L2_relative_error)\n",
    "    sheet1.write(ID, 25, theta_abs_relative_error)\n",
    "    sheet1.write(ID, 26, theta_max_error)\n",
    "    sheet1.write(ID, 27, psi_L2_relative_error)\n",
    "    sheet1.write(ID, 28, psi_abs_relative_error)\n",
    "    sheet1.write(ID, 29, psi_max_error)\n",
    "\n",
    "    sheet1.write(ID, 30, str(loss_data_1_BFGS[-1]))\n",
    "    sheet1.write(ID, 31, str(loss_data_2_BFGS[-1]))\n",
    "    sheet1.write(ID, 32, str(loss_res_1_BFGS[-1]))\n",
    "    sheet1.write(ID, 33, str(loss_res_2_BFGS[-1]))\n",
    "    sheet1.write(ID, 34, str(loss_res_int_BFGS[-1]))\n",
    "    sheet1.write(ID, 35, str(loss_psi_int_BFGS[-1]))\n",
    "    sheet1.write(ID, 36, str(loss_qz_int_BFGS[-1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"example-inverse\"\n",
    "wb = xlwt.Workbook()\n",
    "sheet1 = wb.add_sheet(f'{folder}')\n",
    "sheet1.write(0, 0,\"ID\")\n",
    "sheet1.write(0, 1, \"random_seed\")\n",
    "sheet1.write(0, 2, \"number_layers_1\")\n",
    "sheet1.write(0, 3, \"number_units_1\")\n",
    "sheet1.write(0, 4, \"number_layers_2\")\n",
    "sheet1.write(0, 5, \"number_units_2\")\n",
    "sheet1.write(0, 6, \"LAA\")\n",
    "sheet1.write(0, 7, \"number_res_1\")\n",
    "sheet1.write(0, 8, \"number_res_2\")\n",
    "sheet1.write(0, 9, \"data_depth\")\n",
    "sheet1.write(0, 10, \"number_int\")\n",
    "sheet1.write(0, 11, \"weight_values_data_1\")\n",
    "sheet1.write(0, 12, \"weight_values_data_2\")\n",
    "sheet1.write(0, 13, \"weight_values_res_2\")\n",
    "sheet1.write(0, 14, \"weight_values_int\")\n",
    "sheet1.write(0, 15, \"weight_values_psi_int\")\n",
    "sheet1.write(0, 16, \"weight_values_qz_int\")\n",
    "sheet1.write(0, 17, \"ALR\")\n",
    "sheet1.write(0, 18, \"iterations\")\n",
    "sheet1.write(0, 19, \"batch_size\")\n",
    "sheet1.write(0, 20, \"RAR\")\n",
    "sheet1.write(0, 21, \"new_residual_points\")\n",
    "sheet1.write(0, 22, \"time_Adam\")\n",
    "sheet1.write(0, 23, \"time_BFGS\")\n",
    "sheet1.write(0, 24, \"theta_L2_relative_error\")\n",
    "sheet1.write(0, 25, \"theta_abs_relative_error\")\n",
    "sheet1.write(0, 26, \"theta_max_error\")\n",
    "sheet1.write(0, 27, \"psi_L2_relative_error\")\n",
    "sheet1.write(0, 28, \"psi_abs_relative_error\")\n",
    "sheet1.write(0, 29, \"psi_max_error\")\n",
    "sheet1.write(0, 30,  \"loss_data_1\")\n",
    "sheet1.write(0, 31,  \"loss_data_2\")\n",
    "sheet1.write(0, 32,  \"loss_res_1\")\n",
    "sheet1.write(0, 33,  \"loss_res_2\")\n",
    "sheet1.write(0, 34,  \"loss_res_int\")\n",
    "sheet1.write(0, 35,  \"loss_psi_int\")\n",
    "sheet1.write(0, 36,  \"loss_qz_int\")\n",
    "\n",
    "random_seed = 1\n",
    "data_depth_list = [[10, 50, 90, 130, 170]] # measurement data at -1, -5, -9, -13, -17 cm\n",
    "ID = 1\n",
    "\n",
    "for i in range(len(random_seed_list)):\n",
    "    for j in range(len(data_depth_list)):\n",
    "        main_loop(folder, ID, random_seed, data_depth_list[j])\n",
    "        ID += 1\n",
    "        wb.save(f'../results/inverse/{folder}/summary.xls')"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "UFnets_Srivastva_case3_final4.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
